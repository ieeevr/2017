

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Posters</title>
  
  <meta name="author" content="">

  <!-- Enable responsive viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Bootstrap styles -->
  <link href="/2017/assets/themes/ieee_vr_2017/css/style.css" rel="stylesheet"/>
  <!-- Sticky Footer -->
  <link href="/2017/assets/themes/ieee_vr_2017/css/bs-sticky-footer.css" rel="stylesheet">

  

  <!-- Custom styles -->
  <!--<link href="/2017/assets/themes/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">-->

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <!-- Update these with your own images
  <link rel="shortcut icon" href="images/favicon.ico">
  <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
  <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
-->

<!-- atom & rss feed -->
  <link href="" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
  <link href="" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">


</head>

<body>


  <div id="wrap">
    <nav class="navbar navbar-default navbar-fixed-top" role="navigation">
      <!-- Brand and toggle get grouped for better mobile display -->
      <div class="navbar-header">
        <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#jb-navbar-collapse">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="/2017/">IEEE Virtual Reality 2017</a>
      </div>

      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse navbar-collapse" id="jb-navbar-collapse">
        
	

<ul class="nav navbar-nav">
    
				
				<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Program<span class="caret"></span></a>
						
	<!---->
	

<ul class="dropdown-menu">
    
				
        <li class=" ">
            <a href="/2017/program/overview.html">Overview</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/keynotes.html">Keynotes</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/papers.html">Papers</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/panels.html">Panels</a>

        </li>
				
    
				
        <li class="active ">
            <a href="/2017/program/posters.html">Posters</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/demos.html">Research Demos</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/videos.html">Videos</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/tutorials.html">Tutorials</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/workshops.html">Workshops</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/dc.html">Doctoral Consortium</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/exhibitors.html">Exhibitors</a>

        </li>
				
    
</ul>

				</li>
				
    
				
				<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Attend<span class="caret"></span></a>
						
	<!---->
	

<ul class="dropdown-menu">
    
				
        <li class=" ">
            <a href="/2017/attend/registration.html">Registration</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/attend/accommodations.html">Accomodations</a>

        </li>
				
    
</ul>

				</li>
				
    
				
				<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Contribute<span class="caret"></span></a>
						
	<!---->
	

<ul class="dropdown-menu">
    
				
        <li class=" ">
            <a href="/2017/contribute/papers.html">Call for Papers</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/panels.html">Call for Panels</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/posters.html">Call for Posters</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/tutorials.html">Call for Tutorials</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/researchDemos.html">Call for Research Demos</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/workshops.html">Call for Workshops</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/videos.html">Call for Videos</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/docConsortium.html">Call for Doctoral Consortium</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/studentVolunteers.html">Call for Student Volunteers</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/exhibitors.html">Exhibitors and Supporters</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/formatting.html">Formatting Guidelines</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/contribute/presentation.html">Presentation Guidelines</a>

        </li>
				
    
</ul>

				</li>
				
    
				
				<li class="dropdown">
						<a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Committees<span class="caret"></span></a>
						
	<!---->
	

<ul class="dropdown-menu">
    
				
        <li class=" ">
            <a href="/2017/committees/conference.html">Conference Committee</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/committees/steering.html">Steering Committee</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/committees/program.html">Program Committee</a>

        </li>
				
    
</ul>

				</li>
				
    
				
        <li class=" ">
            <a href="/2017/awards/">Awards</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/past-conferences/">Past Conferences</a>

        </li>
				
    
				
        <li class=" ">
            <a href="/2017/program/3dui.html">3DUI 2017</a>

        </li>
				
    
</ul>

      </div><!-- /.navbar-collapse -->
    </nav>
    <div class="container-fluid fill-height">
      <div class="row">
        <div class="row-lg-height row-md-height">
          <div id="logoandsponsors" class="col-lg-5 col-md-6 col-sm-12 col-lg-height col-md-height">
            <div style="width=100%; height: 450px; background-color: #000; margin-left: -15px;" class="hidden-xs hidden-sm">
              <img src="/2017/assets/themes/ieee_vr_2017/images/palmtree-large-transparent-white-text-maroon-v2.png"
              width="491" height="auto" alt="2017 IEEE VR Los Angeles logo"
              class="pull-right" style="margin-top: 50px; margin-bottom: 30px; margin-right: 45px;"/>
            </div>
            <div style="width=100%; background-color: #000; margin-left: -15px; margin-right: -15px;" class="visible-sm">
              <img src="/2017/assets/themes/ieee_vr_2017/images/palmtree-large-transparent-white-text-maroon-v2.png"
              width="491" height="auto" alt="2017 IEEE VR Los Angeles logo"
              class="center-block" style="padding-top: 50px; padding-bottom: 30px;"/>
            </div>
            <div style="width=100%; background-color: #000; margin-left: -15px; margin-right:-15px;" class="visible-xs">
              <img src="/2017/assets/themes/ieee_vr_2017/images/palmtree-large-transparent-white-text-maroon-v2.png"
              width="60%" height="auto" alt="2017 IEEE VR Los Angeles logo"
              class="center-block" style="vertical-align: middle; padding-top: 50px; padding-bottom: 30px"/>
            </div>

            <div class="row">
              <!-- Sponsor and social media (or other "float left column" content lives here for LG and MD windows)-->
              <div class="col-lg-12 col-md-12 pull-right visible-lg visible-md">
                <img src="/2017/assets/themes/ieee_vr_2017/images/ieee-cs-logo-white-transparent-stacked.png"
                width="150" height="54" style="margin-top: 25px; margin-bottom: 25px; margin-right: 40px" alt="IEEE Computer Society" class="pull-right"/>
                <img src="/2017/assets/themes/ieee_vr_2017/images/ieee-logo-white-transparent.png"
                width="150" height="44" style="margin-top: 25px; margin-bottom: 25px; margin-right: 40px" alt="IEEE" class="pull-right"/>
				<a href="https://twitter.com/ieeevr"><img style="height: 40px; margin-top: 25px; margin-bottom: 25px; margin-right: 40px; margin-left: 2px;" src="/2017/img/twitter.png" class="pull-right"/></a>
				<a href="https://facebook.com/ieeevr"><img style="height: 40px; margin-top: 25px; margin-bottom: 25px; margin-right: 2px;" src="/2017/img/facebook.png" class="pull-right"/></a>
              </div>
            </div>
			
			<div class="row">
              <!-- Sponsor and social media (or other "float left column" content lives here for LG and MD windows)-->
              <div class="col-lg-12 col-md-12 pull-right visible-lg visible-md" style="text-align: center; width: 400px; margin-top: 30px;  margin-bottom: 2px; margin-right: 100px; background-color: #000; color: #fff">
				<h4><b>Exhibitors and Supporters</b></h4>
              </div>
            </div>
			
			<div class="row">
              <!-- Sponsor and social media (or other "float left column" content lives here for LG and MD windows)-->
              <div class="col-lg-12 col-md-12 pull-right visible-lg visible-md" style="text-align: center; width: 300px; margin-right: 150px; margin-bottom: 50px; background-color: #fff; padding-left: 60px; padding-right: 60px; padding-bottom: 30px; padding-top: 10px;">
				<h4>Silver Level</h4>


<img src="/2017/img/exhibitors/ART.jpg" alt="logo" style="width: 85%"/>

<br><br>

<img src="/2017/img/exhibitors/phasespace_logo.jpg" alt="logo" style="width: 100%"/>

<br><br><br>

<h4>Bronze Level</h4>

<img src="/2017/img/exhibitors/ARL.jpg" alt="logo" style="width: 60%"/>

<br><br>

<img src="/2017/img/exhibitors/DigitalProjection.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/Haption.jpg" alt="logo" style="width: 80%"/>

<br><br>

<img src="/2017/img/exhibitors/MiddleVR.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/Polhemus.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/Technicolor.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/Vicon.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/logo_vr_on.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/vrvana.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/worldviz.jpg" alt="logo" style="width: 100%"/>

<br><br><br>

<h4>Event Supporters</h4>

<img src="/2017/img/exhibitors/ICT.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/postmedia.jpg" alt="logo" style="width: 100%"/>

<br><br><br>

<h4>Publisher</h4>

<img src="/2017/img/exhibitors/Frontiers.jpg" alt="logo" style="width: 100%"/>


<br><br><br>


<h4>Other Supporters</h4>

<img src="/2017/img/exhibitors/oben.jpg" alt="logo" style="width: 100%"/>

<br><br>

<img src="/2017/img/exhibitors/UTDallas.jpg" alt="logo" style="width: 100%"/>


              </div>
            </div>
			
			
          </div>
          <div class="col-lg-5 col-md-6 col-sm-12 col-lg-height col-md-height col-lg-top col-md-top" style="background-color: #fff; padding-top: 20px; padding-left: 40px; padding-right: 40px; padding-bottom: 40px; box-shadow: inset 0 0 10px 10px #000;">
            <h1 id="posters">Posters</h1>

<p><br /></p>

<div class="row">
    <div class="col-xs-12">
        <table class="table-responsive program-table">
            <tr>
                <td class="program-days"><h4>Monday<br />March 20</h4></td>
                <td class="program-days"><h4>Tuesday<br />March 21</h4></td>
            </tr>
			<tr>
                <td class="program-vr-session">
					<div class="program-time">10:00am - 10:30am</div><br />
					<a href="#Poster Session A">Poster Viewing A</a>
				</td>
				<td class="program-vr-session">
					<div class="program-time">10:00am - 10:30am</div><br />
					<a href="#Poster Session B">Poster Viewing B</a>
				</td>
            </tr>
			<tr>
                <td class="program-vr-session">
					<div class="program-time">2:40pm - 3:30pm</div><br />
					<a href="#Poster Session A">Poster Fast Forward A</a>
				</td>
				<td class="program-vr-session">
					<div class="program-time">2:40pm - 3:30pm</div><br />
					<a href="#Poster Session B">Poster Fast Forward B</a>
				</td>
            </tr>
			<tr>
                <td class="program-vr-session">
					<div class="program-time">3:30pm - 4:00pm</div><br />
					<a href="#Poster Session A">Poster Viewing A</a>
				</td>
				<td class="program-vr-session">
					<div class="program-time">3:30pm - 4:00pm</div><br />
					<a href="#Poster Session B">Poster Viewing B</a>
				</td>
            </tr>
        </table>
    </div>
</div>

<p><br /><br /></p>

<h4 id="vote-for-the-best-posterhttpsgooglformsizqgcx9a8xn1bdwo1"><a href="https://goo.gl/forms/IzQgcX9a8xN1bDwo1">Vote for the Best Poster!</a></h4>

<p><a name="Poster Session A"> </a></p>

<h2 id="poster-session-a">Poster Session A</h2>

<h4 id="monday-march-20">Monday, March 20</h4>

<p><a name="Attention Guidance for Immersive Video Content in Head-Mounted Displays"> </a></p>

<h4 id="attention-guidance-for-immersive-video-content-in-head-mounted-displays">Attention Guidance for Immersive Video Content in Head-Mounted Displays</h4>

<p><em>Fabien Danieau, Antoine Guillo, and Renaud Doré</em></p>

<p>Abstract: Immersive videos allow users to freely explore 4 π steradian scenes within head-mounted displays (HMD), leading to a strong feeling of immersion. However users may miss important elements of the narrative if not facing them. Hence, we propose four visual effects to guide the user’s attention. After an informal pilot study, two of the most efficient effects were evaluated through a user study. Results show that our approach has potential but it remains challenging to implicitly drive the user’s attention outside of the field of view.</p>

<p><a name="A System for Creating Virtual Reality Content from Make-Believe Games"> </a></p>

<h4 id="a-system-for-creating-virtual-reality-content-from-make-believe-games">A System for Creating Virtual Reality Content from Make-Believe Games</h4>

<p><em>Adela Barbulescu, Maxime Garcia, Antoine Begault, Laurence Boissieux, Marie-Paule Cani, Maxime Portaz, Alexis Viand, Romain Dulery, Pierre Heinish, Remi Ronfard, and Dominique Vaufreydaz</em></p>

<p>Abstract: Pretend play is a storytelling technique, naturally used from very young ages, which relies on object substitution to represent the characters of the imagined story. We propose a system which assists the storyteller by generating a virtualized story from a recorded dialogue performed with 3D printed figurines. We capture the gestures and facial expressions of the storyteller using Kinect cameras and IMU sensors and transfer them to their virtual counterparts in the story-world. As a proof-of-concept, we demonstrate our system with an improvised story involving a prince and a witch, which was successfully recorded and transferred into 3D animation.</p>

<p><a name="High-Definition Wireless Personal Area Tracking using AC Magnetic Field for Virtual Reality"> </a></p>

<h4 id="high-definition-wireless-personal-area-tracking-using-ac-magnetic-field-for-virtual-reality">High-Definition Wireless Personal Area Tracking using AC Magnetic Field for Virtual Reality</h4>

<p><em>Mohit Singh and Byunghoo Jung</em></p>

<p>Abstract: This paper presents an AC magnetic field based High-Definition Personal Area Tracking (PAT) system. A low-power transmitter antenna acts as a reference for three tracker modules. One module, attached to the Head Mount Display (HMD), tracks the position and orientation of user's head and the other two hand-held modules act as an interface device (like virtual hands) in Virtual Reality. This precise, low power, low latency, non-line-of-sight system provides an easy-to-use human-computer interface. The system achieves a precision of 1 mm in position with 0.1 degree in orientation and an accuracy of 20 cm in position at a distance of 2 m from the antenna. The transmitter and the receiver consume 5 W and 0.4 W of power, respectively, providing 140 updates/sec with 11 ms of latency.</p>

<p><a name="HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection"> </a></p>

<h4 id="hysar-hybrid-material-rendering-by-an-optical-see-through-head-mounted-display-with-spatial-augmented-reality-projection">HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection</h4>

<p><em>Yuichi Hiroi, Yuta Itoh, Takumi Hamasaki, Daisuke Iwai, and Maki Sugimoto</em></p>

<p>Abstract: We propose a hybrid SAR concept combining a projector and Optical See-Through Head-Mounted Displays (OST-HMD). Our proposed hybrid SAR system utilizes OST-HMD as an extra rendering layer to render a view-dependent property in OST-HMDs according to the viewer's viewpoint. Combined with view-independent components created by a static projector, the viewer can see richer material contents. Unlike conventional SAR systems, our system theoretically allows unlimited number of viewers seeing enhanced contents in the same space while keeping the existing SAR experiences. Furthermore, the system enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With a proof-of-concept system that consists of a projector and an OST-HMD, we qualitatively demonstrate that our system successfully creates hybrid rendering on a hemisphere object from five horizontal viewpoints. Our quantitative evaluation also shows that our system increases the dynamic range by 2.1 times and the maximum intensity by 1.9 times compared to an ordinary SAR system.</p>

<p><a name="Monocular Focus Estimation Method for a Freely-Orienting Eye using Purkinje-Sanson Images"> </a></p>

<h4 id="monocular-focus-estimation-method-for-a-freely-orienting-eye-using-purkinje-sanson-images">Monocular Focus Estimation Method for a Freely-Orienting Eye using Purkinje-Sanson Images</h4>

<p><em>Yuta Itoh, Jason Orlosky, Kiyoshi Kiyokawa, Toshiyuki Amano, and Maki Sugimoto</em></p>

<p>Abstract: We present a method for focal distance estimation of a freely-orienting eye using Purkinje-Sanson (PS) images, which are reflections of light on the inner structures of the eye. Using an infrared camera with a rigidly-fixed LED, our method creates an estimation model based on 3D gaze and the distance between reflections in the PS images that occur on the corneal surface and anterior surface of the eye lens. The distance between these two reflections changes with focus, so we associate that information to the focal distance on a user. Unlike conventional methods that mainly relies on 2D pupil size which is sensitive to scene lighting and the fourth PS image, our method detects the third PS image which is more representative of accommodation. Our feasibility study on a single user with a focal range from 15-45 cm shows that our method achieves mean and median absolute errors of 3.15 and 1.93 cm for a 10-degree viewing angle. The study shows that our method is also tolerant against environment lighting changes.</p>

<p><a name="Lean into It: Exploring Leaning-Based Motion Cueing Interfaces for Virtual Reality Movement"> </a></p>

<h4 id="lean-into-it-exploring-leaning-based-motion-cueing-interfaces-for-virtual-reality-movement">Lean into It: Exploring Leaning-Based Motion Cueing Interfaces for Virtual Reality Movement</h4>

<p><em>Alexandra Kitson, Abraham M. Hashemian, Ekaterina R. Stepanova, Ernst Kruijff, and Bernhard E. Riecke</em></p>

<p>Abstract: We describe here a pilot user study comparing five different locomotion interfaces for virtual reality (VR) locomotion. We compared a standard non-motion cueing interface, Joystick, with four leaning-based seated motion-cueing interfaces: NaviChair, MuvMan, Head-Directed and Swivel Chair. The aim of this mixed methods study was to investigate the usability and user experience of each interface, in order to better understand relevant factors and guide the design of future ground-based VR locomotion interfaces. We asked participants to give talk-aloud feedback and simultaneously recorded their responses while they were performing a search task in VR. Afterwards, participants completed an online questionnaire. Although the Joystick was rated as more comfortable and precise than the other interfaces, the leaning-based interfaces showed a trend to provide more enjoyment and a greater sense of self-motion. There were also potential issues of using velocity-control for rotations in leaning-based interfaces when using HMDs instead of stationary displays. Developers need to focus on improving the controllability and perceived safety of these seated motion cueing interfaces.</p>

<p><a name="Biomechanical Analysis of (Non-)Isometric Virtual Walking of Older Adults"> </a></p>

<h4 id="biomechanical-analysis-of-non-isometric-virtual-walking-of-older-adults">Biomechanical Analysis of (Non-)Isometric Virtual Walking of Older Adults</h4>

<p><em>Omar Janeh, Eike Langbehn, Frank Steinicke, Gerd Bruder, Alessandro Gulberti, and Monika Poetter-Nerger</em></p>

<p>Abstract: Our study investigates the effects of (non-)isometric mappings between physical movements and virtual motions in the virtual environment (VE) on walking biomechanics of older adults. Three primary domains (pace, base of support and phase) of spatio-temporal and temporo-phasic parameters were used to evaluate gait performance. Our results show similar results in pace and phasic domains when older adults walk in the VE in the isometric mapping condition compared to the corresponding parameters in the real world. We found significant differences in base of support for our user group between walking in the VE and real world. For non-isometric mappings we found an increased divergence of gait parameters in all domains correlating with the up- or down-scaled velocity of visual self-motion feedback.</p>

<p><a name="Robust Optical See-Through Head-Mounted Display Calibration: Taking Anisotropic Nature of User Interaction Errors into Account"> </a></p>

<h4 id="robust-optical-see-through-head-mounted-display-calibration-taking-anisotropic-nature-of-user-interaction-errors-into-account">Robust Optical See-Through Head-Mounted Display Calibration: Taking Anisotropic Nature of User Interaction Errors into Account</h4>

<p><em>Ehsan Azimi, Long Qian, Peter Kazanzides, and Nassir Navab</em></p>

<p>Abstract: Uncertainty in measurement of point correspondences negatively affects the accuracy and precision in the calibration of head-mounted displays (HMD). Such errors depend on the sensors and pose estimation for video see-through HMD. For optical see-through systems, it additionally depends on the user's head motion and hand-eye coordination. Therefore, the distribution of alignment errors for optical see-through calibration are not isotropic, and one can estimate its process specific or user specific distribution based on interaction requirements of a given calibration process and the user's measurable head motion and hand-eye coordination characteristics. Current calibration methods, however, mostly utilize the DLT method which minimizes Euclidean distances for HMD projection matrix estimation, disregarding the anisotropicity in the alignment errors. We will show how to utilize the error covariance in order to take the anisotropic nature of error distribution into account. The main hypothesis of this study is that using Mahalonobis distance within the nonlinear optimization can improve the accuracy of the HMD calibration. To cover a wide range of possible realistic scenarios, several simulations were performed with variation in the extent of the anisotropicity in the input data along with other parameters. The simulation results indicate that our new method outperforms the standard DLT method both in accuracy and precision, and is more robust against user alignment errors. To the best of our knowledge, this is the first time that anisotropic noise has been accommodated in the optical see-through HMD calibration.</p>

<p><a name="6 Degrees-of-Freedom Manipulation with a Transparent, Tangible Object in World-Fixed Virtual Reality Displays"> </a></p>

<h4 id="degrees-of-freedom-manipulation-with-a-transparent-tangible-object-in-world-fixed-virtual-reality-displays">6 Degrees-of-Freedom Manipulation with a Transparent, Tangible Object in World-Fixed Virtual Reality Displays</h4>

<p><em>David J. Zielinski, Derek Nankivil, and Regis Kopper</em></p>

<p>Abstract: We propose Specimen Box, an interaction technique that allows world-fixed display (such as CAVEs) users to naturally hold a plausible physical object while manipulating virtual content inside it. This virtual content is rendered based on the tracked position of the box. Specimen Box provides the weight and tactile feel of an actual object and does not occlude rendered objects in the scene. The end result is that the user sees the virtual content as if it exists inside the clear physical box. We conducted a user study which involved a cognitively loaded inspection task requiring extensive manipulation of the box. We compared Specimen Box to Grab-and-Twirl, a naturalistic bimanual manipulation technique that closely mimics the mechanics of our proposed technique. Results show that performance was significantly faster with Specimen Box. Further, performance of the control technique was positively affected by experience with Specimen Box.</p>

<p><a name="Proposal of a Spectral Random Dots Marker using Local Feature for Posture Estimation"> </a></p>

<h4 id="proposal-of-a-spectral-random-dots-marker-using-local-feature-for-posture-estimation">Proposal of a Spectral Random Dots Marker using Local Feature for Posture Estimation</h4>

<p><em>Norimasa Kobori, Daisuke Deguchi, Ichiro Ide, and Hiroshi Murase</em></p>

<p>Abstract: We propose a novel marker for robot's grasping task which has the following three aspects: (i) it is easy-to-find in a cluttered background, (ii) it is calculable for its posture (iii) its size is compact. The proposed marker is composed of a random dots pattern, and uses keypoint detection and a scale estimation by Spectral SIFT for dots detection and data decoding. The data is encoded by the scale size of dots, and the same dots in the marker work for both marker detection and data decoding. As a result, the proposed marker size can be compact. We confirmed the effectiveness of the proposed marker through experiments.</p>

<p><a name="All Are Welcome: Using VR Ethnography to Explore Harassment Behavior in Immersive Social Virtual Reality"> </a></p>

<h4 id="all-are-welcome-using-vr-ethnography-to-explore-harassment-behavior-in-immersive-social-virtual-reality">All Are Welcome: Using VR Ethnography to Explore Harassment Behavior in Immersive Social Virtual Reality</h4>

<p><em>Ketaki Shriram and Raz Schwartz</em></p>

<p>Abstract: The growing ubiquity of VR headsets has given rise to questions around harassment in social virtual reality. This paper presents two studies. In the first, a pilot ethnographic study, users were interviewed in immersive social virtual reality about their experiences and behaviors in these spaces. Harassment was occasional, and those in female avatars reported more harassment than those in male avatars. In Study Two, a quantitative survey was conducted to validate ethnographic results. A large percentage of users witness harassment in virtual reality. These studies provide mixed methods insight of user demographics and behaviors in the relatively new social VR space.</p>

<p><a name="Study of Interaction Fidelity for Two Viewpoint Changing Techniques in a Virtual Biopsy Trainer"> </a></p>

<h4 id="study-of-interaction-fidelity-for-two-viewpoint-changing-techniques-in-a-virtual-biopsy-trainer">Study of Interaction Fidelity for Two Viewpoint Changing Techniques in a Virtual Biopsy Trainer</h4>

<p><em>Aylen Ricca, Amine Chellali, and Samir Otmane</em></p>

<p>Abstract: Virtual Reality simulators are increasingly used for training novice surgeons. However, there is currently a lack of guidelines for achieving interaction fidelity for these systems. In this paper, we present the design of two navigation techniques for a needle insertion trainer. The two techniques were analyzed using a state-of-the-art fidelity framework to determine their level of interaction fidelity. A user study comparing both techniques suggests that the higher fidelity technique is more suited as a navigation technique for the needle insertion virtual trainer.</p>

<p><a name="Conditions Influencing Perception of Wind Direction by the Head"> </a></p>

<h4 id="conditions-influencing-perception-of-wind-direction-by-the-head">Conditions Influencing Perception of Wind Direction by the Head</h4>

<p><em>Takuya Nakano and Yasuyuki Yanagida</em></p>

<p>Abstract: Recently, several virtual reality (VR) systems using wind have been built to enhance the user’s sense of presence. If wind is used, users need not use additional devices, and some studies have concluded that using wind and presenting a video simultaneously improves this sense of presence. In these studies, however, wind sources were sparsely arranged; with such an arrangement, it is unclear whether an accurate environment was reproduced. A number of variables including gender, age, and the facial hit rate may affect the perception of wind direction. In the proposed study, we have examined the effect of these variables on perception of wind direction by the head.</p>

<p><a name="The AR-Rift 2 Prototype"> </a></p>

<h4 id="the-ar-rift-2-prototype">The AR-Rift 2 Prototype</h4>

<p><em>Anthony Steed, Yonathan Widya Adipradana, and Sebastian Friston</em></p>

<p>Abstract: Video see-through augmented reality (VSAR) is an effective way of combing real and virtual scenes for head-mounted human computer interfaces. In this paper we present the AR-Rift 2 system, a cost-effective prototype VSAR system based around the Oculus Rift CV1 head-mounted display (HMD). Current consumer camera systems however typically have latencies far higher than the rendering pipeline of current consumer HMDs. They also have lower update rate than the display. We thus measure the latency of the video and implement a simple image-warping method to ensure smooth movement of the video.</p>

<p><a name="Motor Adaptation in Response to Scaling and Diminished Feedback in Virtual Reality"> </a></p>

<h4 id="motor-adaptation-in-response-to-scaling-and-diminished-feedback-in-virtual-reality">Motor Adaptation in Response to Scaling and Diminished Feedback in Virtual Reality</h4>

<p><em>David M. Krum, Thai Phan, and Sin-Hwa Kang</em></p>

<p>Abstract: As interaction techniques involving scaling of motor space in virtual reality are becoming more prevalent, it is important to understand how individuals adapt to such scalings and how they re-adapt back to non-scaled norms. This preliminary work examines how individuals, performing a targeted ball throwing task, adapted to addition and removal of a translational scaling of the ball's forward flight. This was examined under various conditions: flight of the ball shown with no delay, hidden flight of the ball with no delay, and hidden flight with a 2 second delay. Hiding the ball’s flight, as well as the delay, created disruptions in the ability of the participants to perform the task and adapt to new scaling conditions.</p>

<p><a name="Separation of Reflective and Fluorescent Components using the Color Mixing Matrix"> </a></p>

<h4 id="separation-of-reflective-and-fluorescent-components-using-the-color-mixing-matrix">Separation of Reflective and Fluorescent Components using the Color Mixing Matrix</h4>

<p><em>Isao Shimana and Toshiyuki Amano</em></p>

<p>Abstract: In the field of SAR, the projector-camera system has been well studied; its radiometric model can be easily described by a color mixing matrix.  Many SAR applications have proposed and created by using this model. However, this model can be used for reflectance component, but not for fluorescence component. In this paper, we propose RKS Projector–Camera response model for separating of the color mixing matrix’s reflectance components and fluorescence components and describe how to decompose them.</p>

<p><a name="Group Immersive Education with Digital Fulldome Planetariums"> </a></p>

<h4 id="group-immersive-education-with-digital-fulldome-planetariums">Group Immersive Education with Digital Fulldome Planetariums</h4>

<p><em>Ka Chun Yu, Kamran Sahami, Victoria Sahami, Larry Sessions, and Grant Denn</em></p>

<p>Abstract: Although fulldome video digital theaters evolved from traditional planetariums, they are more akin to virtual reality (VR) theaters that create large-scale, group immersive experiences. In order to help understand how immersion and wide fields-of-view (FOV) impact learning, we studied the use of visualizations on topics that do and do not require spatial understanding in astronomy classes. We find a significant difference between students who viewed visualizations in the dome versus those that saw non-immersive content in their classrooms, with the former showing the greatest retention. Our results suggest that immersive visuals help free up cognitive resources that can be used to build mental models requiring spatial understanding, and the physical display size combined with the wide FOV may result in greater attention. Although fulldome is a complementary medium to traditional VR, our results have implications for future head-mounted displays.</p>

<p><a name="MR Sand Table: Mixing Real-Time Video Streaming in Physical Models"> </a></p>

<h4 id="mr-sand-table-mixing-real-time-video-streaming-in-physical-models">MR Sand Table: Mixing Real-Time Video Streaming in Physical Models</h4>

<p><em>Zhong Zhou, Zhiyi Bian, and Zheng Zhuo</em></p>

<p>Abstract: A novel prototype of MR (Mixed Reality) Sand Table is presented in this paper, that fuses multiple real-time video streaming into a physically united view. The main processes include geometric calibration and alignment, image blending and the final projection. Firstly we proposed a two-step MR alignment scheme which estimates the transform matrix between input video streaming and the sand table for coarse alignment, and deforms the input frame using moving least squares for accurate alignment. To overcome the video border distinction problem, we make a border-adaptive image stitching with brightness diffusion to merge the overlapping area. With the projection, the video area can be mixed into the sand table in real-time to provide a live physical mixed reality model. We build a prototype to demonstrate the effectiveness of the proposed method. This design could also be easily extended to large size with help of multiple projectors. The system proposed in this paper supports multiple user interaction in a broad area of applications such as surveillance, demonstration, action preview and discussion assistances.</p>

<p><a name="Mobile Collaborative Mixed Reality for Supporting Scientific Inquiry and Visualization of Earth Science Data"> </a></p>

<h4 id="mobile-collaborative-mixed-reality-for-supporting-scientific-inquiry-and-visualization-of-earth-science-data">Mobile Collaborative Mixed Reality for Supporting Scientific Inquiry and Visualization of Earth Science Data</h4>

<p><em>Suya You and Charles K. Thompson</em></p>

<p>Abstract: This work seeks to apply the emerging virtual and mixed reality techniques to visual exploration and visualization of earth science data. A novel system is developed to facilitate a collaborative mixed reality visualization, enabling both in-situ and off-site users to simultaneously interact with and visualize science data within mixed reality realm. We implement the prototype system in the context of visualizing earth terrain data. We report our current prototype effort and preliminary results.</p>

<p><a name="Augmenting Creative Design Thinking using Networks of Concepts"> </a></p>

<h4 id="augmenting-creative-design-thinking-using-networks-of-concepts">Augmenting Creative Design Thinking using Networks of Concepts</h4>

<p><em>Georgi V. Georgiev, Kaori Yamada, Toshiharu Taura, Vassilis Kostakos, Matti Pouke, Sylvia Tzvetanova Yung, and Timo Ojala</em></p>

<p>Abstract: Here we propose an interactive system to augment creative design thinking using networks of concepts in a virtual reality environment. We discuss how to augment the human capacity to be creative through dynamic suggestions providing new and original ideas, based on specific semantic network characteristics. We outline directions to explore the structures of the concept network and their connection to creative concept generation. It is expected that augmented creative thinking will allow the user to have more original ideas and thus be more innovative.</p>

<p><a name="RIDE: Region-Induced Data Enhancement Method for Dynamic Calibration of Optical See-Through Head-Mounted Displays"> </a></p>

<h4 id="ride-region-induced-data-enhancement-method-for-dynamic-calibration-of-optical-see-through-head-mounted-displays">RIDE: Region-Induced Data Enhancement Method for Dynamic Calibration of Optical See-Through Head-Mounted Displays</h4>

<p><em>Zhenliang Zhang, Dongdong Weng, Yue Liu, Yongtian Wang, and Xinjun Zhao</em></p>

<p>Abstract: The most commonly used single point active alignment method (SPAAM) is based on a static pinhole camera model, in which it is assumed that both the eye and the HMD are fixed. This leads to a limitation for calibration precision. In this work, we propose a dynamic pinhole camera model according to the fact that the human eye would experience an obvious displacement over the whole calibration process. Based on such a camera model, we propose a new calibration data acquisition method called the region-induced data enhancement (RIDE) to revise the calibration data. The experimental results prove that the proposed dynamic model performs better than the traditional static model in actual calibration.</p>

<p><a name="Turn Physically Curved Paths into Virtual Curved Paths"> </a></p>

<h4 id="turn-physically-curved-paths-into-virtual-curved-paths">Turn Physically Curved Paths into Virtual Curved Paths</h4>

<p><em>Keigo Matsumoto, Takuji Narumi, Yuki Ban, Tomohiro Tanikawa, and Michitaka Hirose</em></p>

<p>Abstract: Redirected walking allows users to explore a large virtual environment while there is a limitation of the room size. Previous works tried to present users straight path in a virtual environment while they walked on a curved path in reality. We expand a previous technique to present users a various curved path in a virtual environment while they walked on a particular curved path or a straight path with/without haptics. Furthermore, we propose a novel estimation methodology to quantify walking paths which user has thought he walked in reality. The data from our experiment shows that users feel walking a various curved path in VR as same as one-to-one mapping condition.</p>

<p><a name="Effects of Using HMDs on Visual Fatigue in Virtual Environments"> </a></p>

<h4 id="effects-of-using-hmds-on-visual-fatigue-in-virtual-environments">Effects of Using HMDs on Visual Fatigue in Virtual Environments</h4>

<p><em>Jie Guo, Dongdong Weng, Henry Been-Lirn Duh, Yue Liu, and Yongtian Wang</em></p>

<p>Abstract: There are few negative effects to make people discomfort using virtual reality systems. In this paper, we investigated the effects of visual fatigue when wearing head-mounted displays (HMD) and compared the results with those from the smartphones. Forty subjects were recruited and divided into two different groups. The visual fatigue scale was measured to assess the subjects’ performance. The results indicated that visual fatigue caused by the conflict of focal distance and vergence distance was less severe than visual fatigue caused by long-term focus without accommodation.</p>

<p><a name="Upright Adjustment of 360 Spherical Panoramas"> </a></p>

<h4 id="upright-adjustment-of-360-spherical-panoramas">Upright Adjustment of 360 Spherical Panoramas</h4>

<p><em>Jinwoong Jung, Joon-Young Lee, Byungmoon Kim, and Seungyong Lee</em></p>

<p>Abstract: With the recent advent of 360 cameras, spherical panorama images are becoming more popular and widely available. In a spherical panorama, alignment of the scene orientation to the image axes is important for providing comfortable and pleasant viewing experiences using VR headsets and traditional displays. This paper presents an automatic framework for upright adjustment of 360 spherical panorama images without any prior information, such as depths and Gyro sensor data. We take the Atlanta world assumption and use the horizontal and vertical lines in the scene to formulate a cost function for upright adjustment. Our method produces visually pleasing results for a variety of real-world spherical panoramas in less than a second.</p>

<p><a name="The Effect of Lip and Arm Synchronization on Embodiment: A Pilot Study"> </a></p>

<h4 id="the-effect-of-lip-and-arm-synchronization-on-embodiment-a-pilot-study">The Effect of Lip and Arm Synchronization on Embodiment: A Pilot Study</h4>

<p><em>Tara Collingwoode-Williams, Marco Gillies, Cade McCall, and Xueni Pan</em></p>

<p>Abstract: We are interested the effect of lip and arm synchronization on body ownership in VR (the illusion that the users own a virtual body). Participants were invited to give a presentation in an HMD, while seeing in a virtual mirror a gender-matched avatar who copied their arm and lip movements in sync and a-sync conditions. We measure participants’ reaction with questionnaires administrated verbally after their presentation while immersed in VR. The result suggested an interaction effect of arm and lip, showing reports of higher level of embodiment with the congruent as compared to the incongruent conditions. Further study is needed to confirm if the same interaction effect can be captured with objective measurements.</p>

<p><a name="Virtual Reality Based Training: Evaluation of User Performance by Capturing Upper Limb Motion"> </a></p>

<h4 id="virtual-reality-based-training-evaluation-of-user-performance-by-capturing-upper-limb-motion">Virtual Reality Based Training: Evaluation of User Performance by Capturing Upper Limb Motion</h4>

<p><em>Ehsan Zahedi, Hadi Rahmat-Khah, Javad Dargahi, and Mehrdad Zadeh</em></p>

<p>Abstract: This paper presents the results of a two-fold study on the incorporation of upper limb's movement into measuring of user performance in a virtual reality (VR) based training simulation. VR simulators have been developed to assess and improve minimally invasive surgery (MIS) skills. While these simulators are currently being used, most skill evaluation methods are limited to measuring and computing performance metrics regarding the MIS tool tip movement. In this study, a VR simulator is developed to measure and analyze the movements of upper limb joints. The movement analysis from the first experiment suggests that the kinematic data of upper limb can be used to discriminate an expert surgeon from a novice trainee. The results from the second experiment show that the motion of non-dominant hand has a significant effect on the performance of dominant hand.</p>

<p><a name="Mechanism of Integrating Force and Vibrotactile Cues for 3D User Interaction within Virtual Environments"> </a></p>

<h4 id="mechanism-of-integrating-force-and-vibrotactile-cues-for-3d-user-interaction-within-virtual-environments">Mechanism of Integrating Force and Vibrotactile Cues for 3D User Interaction within Virtual Environments</h4>

<p><em>Aida Erfanian, Stanley Tarng, Yaoping Hu, Jérémy Plouzeau, and Frédéric Merienne</em></p>

<p>Abstract: Proper integration of sensory cues facilitates 3D user interaction within virtual environments (VEs). Studies showed that the integration of visual and haptic cues follows maximum likelihood estimation (MLE). Little effort focuses however on the mechanism of integrating force and vibrotactile cues. We thus investigated MLE’s suitability for integrating these cues. Within a VE, human users undertook 3D interaction of navigating a flying drone along a high-voltage transmission line for inspection. The users received individual force or vibrotactile cues, and their combinations in collocated and dislocated settings. The users’ task performance including completion time and accuracy was assessed under each individual cue and setting. The presence of the vibrotactile cue promoted a better performance than the force cue alone. This agreed with the applicability of tactile cues for sensing 3D surfaces, herein setting a baseline for using MLE. The task performance under the collocated setting indicated a degree of combining the individual cues. In contrast, the performance under the dislocated setting was alike under the individual vibrotactile cue. These observations imply a possible role of MLE in integrating force and vibrotactile cues for 3D user interaction within VEs.</p>

<p><a name="Socially Immersive Avatar-Based Communication"> </a></p>

<h4 id="socially-immersive-avatar-based-communication">Socially Immersive Avatar-Based Communication</h4>

<p><em>Daniel Roth, Kristoffer Waldow, Marc Erich Latoschik, Arnulph Fuhrmann, and Gary Bente</em></p>

<p>Abstract: In this paper, we present SIAM-C, an avatar-mediated communication  platform to study socially immersive interaction in virtual  environments. The proposed system is capable of tracking, transmitting,  representing body motion, facial expressions, and voice via  virtual avatars and inherits the transmission of human behaviors that  are available in real-life social interactions. Users are immersed using  active stereoscopic rendering projected onto a life-size projection  plane, utilizing the concept of “fish tank” virtual reality (VR).  Our prototype connects two separate rooms and allows for socially  immersive avatar-mediated communication in VR.</p>

<p><a name="A Comparison of Methods for Navigation and Wayfinding in Large Virtual Environments using Walking"> </a></p>

<h4 id="a-comparison-of-methods-for-navigation-and-wayfinding-in-large-virtual-environments-using-walking">A Comparison of Methods for Navigation and Wayfinding in Large Virtual Environments using Walking</h4>

<p><em>Richard A. Paris, Timothy P. McNamara, John J. Rieser, and Bobby Bodenheimer</em></p>

<p>Abstract: Interesting virtual environments that permit free exploration are rarely small. A number of techniques have been developed to allow people to walk in larger virtual spaces than permitted by physical extent of the virtual reality hardware, and in this paper we compare three such methods in terms of how they affect presence and spatial awareness. In our first psychophysical study, we compared two methods of reorientation and one method of redirected walking on subjects' presence and spatial memory while navigating a pre-specified path. Our results suggested no difference between the two methods of reorientation but inferior performance of the redirected walking method. We further compared the two reorientation methods in a second psychophysical study involving free exploration and navigation in a large virtual environment. Our results provide criteria by which the choice of a locomotion method for navigating large virtual environments may be selected.</p>

<p><a name="Immersive Data Interaction for Planetary and Earth Sciences"> </a></p>

<h4 id="immersive-data-interaction-for-planetary-and-earth-sciences">Immersive Data Interaction for Planetary and Earth Sciences</h4>

<p><em>Victor Ardulov and Oleg Pariser</em></p>

<p>Abstract: The Multimission Instrument Processing Laboratory (MIPL) at Jet Propulsion Laboratory (JPL) processes and analyzes, orbital and in-situ instrument data for both planetary and Earth science missions. Presenting 3D data in a meaningful and effective manner is of the utmost importance to furthering scientific research and conducting engineering operations.Visualizing data in an intuitive way by utilizing Virtual Reality (VR), allows users to immersively interact with their data in their respective environments. This paper examines several use-cases, across various missions, instruments, and environments, demonstrating the strengths and insights that VR has to offer scientists.</p>

<p><a name="Bodiless Embodiment: A Descriptive Survey of Avatar Bodily Coherence in First-Wave Consumer VR Applications"> </a></p>

<h4 id="bodiless-embodiment-a-descriptive-survey-of-avatar-bodily-coherence-in-first-wave-consumer-vr-applications">Bodiless Embodiment: A Descriptive Survey of Avatar Bodily Coherence in First-Wave Consumer VR Applications</h4>

<p><em>Dooley Murphy</em></p>

<p>Abstract: This preliminary study surveys whether/which avatar body parts are visible in first-wave consumer virtual reality (VR) applications for the HTC Vive (n = 200). A simple coding schema for assessing avatar bodily coherence (ABC) is piloted and evaluated. Results provide a snapshot of ABC in popular high-end VR applications in Q3 2016. It is reported that 86.5% of sampled items feature fully invisible avatars, 9% depict hands only, and 4.5% feature a head, torso, or legs, but with some degree of bodily incoherence. Findings suggest that users may experience a sense of ownership and/or agency over their virtual actions even in the absence of visible avatar body parts. This informs research questions and hypotheses for future experimental enquiry into how bodily representation may interplay with user cognition, perceived virtual embodiment (body ownership illusion and sense of agency; body schema–image relations), and spatial telepresence. For instance: To what extent/under what conditions do the users of consumer VR systems demonstrate a sense of bodily vulnerability (a drive for bodily preservation) when no virtual body is present/visible?</p>

<p><a name="Curvature Gains in Redirected Walking: A Closer Look"> </a></p>

<h4 id="curvature-gains-in-redirected-walking-a-closer-look">Curvature Gains in Redirected Walking: A Closer Look</h4>

<p><em>Malte Nogalski and Wolfgang Fohl</em></p>

<p>Abstract: This paper summarizes the detailed paths of participants in redirected walking (RDW) curvature gain experiments. The experiments were carried out in a wave field synthesis (WFS) system of 5x6 meters. Some users were blindfolded and had to control their walking by acoustical cues only, others wore an Oculus Rift DK2 which presented them a virtual scenery in addition. A marker at the participant’s head allowed us to record the paths with our high-precision tracking system. The naive assumption of RDW with curvature gains would be that the test persons walk on the circumference of a circle, but the observed walking patterns were much more complex. Test persons showed very individual walking patterns while exploring the virtual environment. Many of these patterns may be explained as a sequence: 1. walk a few steps toward the assumed target position, 2. check for deviations, 3. adjust path to new assumed target position, which results in different patterns of various path curvature. The consequences for the application of RDW techniques are: Curvature gain tries to guide the users on a circular arc: the ”ideal path”, whereas the real paths are mostly outside of the circle of the ideal path. The deviations in the audio-only case are much larger than in the audio-visual case. The measured curvature gain thresholds systematically under-estimate the required walking space, as they do not account for the required extra space for walking outside the circular path.</p>

<p><a name="Catching a Real Ball in Virtual Reality"> </a></p>

<h4 id="catching-a-real-ball-in-virtual-reality">Catching a Real Ball in Virtual Reality</h4>

<p><em>Matthew K. X. J. Pan and Günter Niemeyer</em></p>

<p>Abstract: We present a system enabling users to accurately catch a real ball while immersed in a virtual reality environment. We ex-amine three visualizations: rendering a matching virtual ball, the predicted trajectory of the ball, and a target catching point lying on the predicted trajectory. In our demonstration system, we track the projectile motion of a ball as it is being tossed between users. Using Unscented Kalman Filtering, we generate predictive estimates of the ball’s motion as it approaches the catcher. The predictive assistance visualizations effectively increases the user’s senses but can also alter the user’s strategy in catching.</p>

<p><a name="Towards Usable Underwater Virtual Reality Systems"> </a></p>

<h4 id="towards-usable-underwater-virtual-reality-systems">Towards Usable Underwater Virtual Reality Systems</h4>

<p><em>Raphael Costa, Rongkai Guo, and John Quarles</em></p>

<p>Abstract: The objective of this research is to compare the effectiveness of different tracking devices underwater. There have been few works in aquatic virtual reality (VR) - i.e., VR systems that can be used in a real underwater environment. Moreover, the works that have been done have noted limitations on tracking accuracy. Our initial test results suggest that inertial measurement units work well underwater for orientation tracking but a different approach is needed for position tracking. Towards this goal, we have waterproofed and evaluated several consumer tracking systems intended for gaming to determine the most effective approaches. First, we informally tested infrared systems and fiducial marker based systems, which demonstrated significant limitations of optical approaches. Next, we quantitatively compared inertial measurement units (IMU) and a magnetic tracking system both above water (as a baseline) and underwater. By comparing the devices’ rotation data, we have discovered that the magnetic tracking system implemented by the Razer Hydra is more accurate underwater as compared to a phone-based IMU. This suggests that magnetic tracking systems should be further explored for underwater VR applications.</p>

<p><a name="Development and Evaluation of a Hands-Free Motion Cueing Interface for Ground-Based Navigation"> </a></p>

<h4 id="development-and-evaluation-of-a-hands-free-motion-cueing-interface-for-ground-based-navigation">Development and Evaluation of a Hands-Free Motion Cueing Interface for Ground-Based Navigation</h4>

<p><em>Jacob Freiberg, Alexandra Kitson, and Bernhard E. Riecke</em></p>

<p>Abstract: With affordable high performance VR displays becoming commonplace, users are becoming increasingly aware of the need for well-designed locomotion interfaces that support these displays. After considering the needs of users, we quantitatively evaluated an embodied locomotion interface called the Navichair according to usability needs and fulfillment of system requirements. Specifically, we investigated influences of locomotion interfaces (joystick vs. an embodied motion cueing chair) and display type (HMD vs. projection screen) on a spatial updating pointing task. Our findings indicate that our embodied VR locomotion interface provided users with an immersive experience of a space without requiring a significant investment of set up time. Design lessons and future design goals of our interface are discussed.</p>

<p><a name="Preliminary Exploration: Perceived Egocentric Distance Measures in Room-Scale Spaces using Consumer-Grade Head Mounted Displays"> </a></p>

<h4 id="preliminary-exploration-perceived-egocentric-distance-measures-in-room-scale-spaces-using-consumer-grade-head-mounted-displays">Preliminary Exploration: Perceived Egocentric Distance Measures in Room-Scale Spaces using Consumer-Grade Head Mounted Displays</h4>

<p><em>Alex Peer and Kevin Ponto</em></p>

<p>Abstract: Distance misperception (sometimes, distance compression) in immersive virtual environments is an active area of study, and the recent availability of consumer-grade display and tracking technologies raises new questions: Can misperceptions be measured within the small tracking volumes of consumer-grade technology? Are measures practical within this space directly comparable, or are some preferable to others? Do contemporary displays even induce distance misperceptions? This work explores these questions.</p>

<p><a name="Diminished Reality for Acceleration Stimulus: Motion Sickness Reduction with Vection for Autonomous Driving"> </a></p>

<h4 id="diminished-reality-for-acceleration-stimulus-motion-sickness-reduction-with-vection-for-autonomous-driving">Diminished Reality for Acceleration Stimulus: Motion Sickness Reduction with Vection for Autonomous Driving</h4>

<p><em>Taishi Sawabe, Masayuki Kanbara, and Norihiro Hagita</em></p>

<p>Abstract: This paper presents an approach for motion sickness reduction while riding an autonomous vehicle. It proposes the Diminished Reality (DR) method for an acceleration stimulus to reduce motion sickness for the autonomous vehicle. One of the main causes of motion sickness is a repeated acceleration. In order to diminish the acceleration stimulus in the autonomous vehicle, vection illusion is used to induce the user to make a preliminary movement against the real acceleration. The Balance Wii Board is used to measure participant's movement of the center of gravity to verify the effectiveness of the method with vection. The experimental result of 9 participants shows that the proposed method of using vection could reduce acceleration stimulus compared with the conventional method.</p>

<p><a name="Interaction with WebVR 360° Video Player: Comparing Three Interaction Paradigms"> </a></p>

<h4 id="interaction-with-webvr-360-video-player-comparing-three-interaction-paradigms">Interaction with WebVR 360° Video Player: Comparing Three Interaction Paradigms</h4>

<p><em>Toni Pakkanen, Jaakko Hakulinen, Tero Jokela, Ismo Rakkolainen, Jari Kangas, Petri Piippo, Roope Raisamo, and Marja Salmimaa</em></p>

<p>Abstract: Immersive 360° video needs new ways of interaction. We compared three different interaction methods to find out which one of them is the most applicable for controlling 360° video playback. The compared methods were: remote control, pointing with head orientation, and hand gestures. A WebVR-based 360° video player was built for the experiment.</p>

<p><a name="Comparing VR and Non-VR Driving Simulations: An Experimental User Study"> </a></p>

<h4 id="comparing-vr-and-non-vr-driving-simulations-an-experimental-user-study">Comparing VR and Non-VR Driving Simulations: An Experimental User Study</h4>

<p><em>Florian Weidner, Anne Hoesch, Sandra Poeschl, and Wolfgang Broll</em></p>

<p>Abstract: Up to now, most driving simulators use either small monitors or large immersive projection setups like 2D/3D screens or a CAVE. The recent improvements of VR-HMDs led to an increased application in driving simulation. However, the influence and comparability of various VR and non-VR displays has been hardly investigated.  We present results of a user study investigating the different influence of non-VR (2D, stereoscopic 3D) and VR (HMD) on physiological responses, simulation sickness, and driving performance within a single driving simulator. In the study, 94 participants performed the Lane Change Task.   Results indicate that a VR-HMD leads to similar data as stereoscopic 3D or 2D screens. We observed no significant difference regarding physiological responses or lane change performance. However, we measured significantly increased simulator sickness in the VR-HMD condition compared to stereoscopic 3D.</p>

<p><a name="Evaluation of Airflow Effect on a VR Walk"> </a></p>

<h4 id="evaluation-of-airflow-effect-on-a-vr-walk">Evaluation of Airflow Effect on a VR Walk</h4>

<p><em>Masato Kurosawa, Ken Ito, Yasushi Ikei, Koichi Hirota, and Michiteru Kitazaki</em></p>

<p>Abstract: The present study investigates the augmentation effect of airflow on the sensation of a virtual reality walk. The intensity of cutaneous sensation evoked by airflow during the real and virtual walk was measured. The airflow stimulus was added to the participant with passive vestibular motion and visual presentation. The result suggests that the sensation of walking was strongly increased by adding the airflow stimulus to the vestibular and optic presentations. The cutaneous sensation of airflow was perceived higher for the sitting participant than during a real walk in both a single and the combined stimuli. The equivalent speed of airflow for the sitting participant was lowered from the airflow speed in the real walk.</p>

<p><a name="The Impact of Transitions on User Experience in Virtual Reality"> </a></p>

<h4 id="the-impact-of-transitions-on-user-experience-in-virtual-reality">The Impact of Transitions on User Experience in Virtual Reality</h4>

<p><em>Liang Men, Nick Bryan-Kinns, Amelia Shivani Hassard, and Zixiang Ma</em></p>

<p>Abstract: In recent years, Virtual Reality (VR) applications have become widely available. An increase in popular interest raises questions about the use of the new medium for communication. While there is a wide variety of literature regarding scene transitions in films, novels and computer games, transitions in VR are not yet widely understood. As a medium that requires a high level of immersion, transitions are a desirable tool. This poster delineates an experiment studying the impact of transitions on user experience of presence in VR.</p>

<p><a name="Coherence Changes Gaze Behavior in Virtual Human Interactions"> </a></p>

<h4 id="coherence-changes-gaze-behavior-in-virtual-human-interactions">Coherence Changes Gaze Behavior in Virtual Human Interactions</h4>

<p><em>Richard Skarbez, Gregory F. Welch, Frederick P. Brooks Jr., and Mary C. Whitton</em></p>

<p>Abstract: We discuss the design and results of an experiment investigating Plausibility Illusion in virtual human (VH) interactions, in particular, the coherence of conversation with a VH. This experiment was performed in combination with another experiment evaluating two display technologies. As that aspect of the study is not relevant to this poster, it will be mentioned only in the Materials section. Participants who interacted with a low-coherence VH looked around the room markedly more than participants interacting with a high-coherence VH, demonstrating that the level of coherence of VHs can have a detectable effect on user behavior and that head and gaze behavior can be used to evaluate the quality of a VH interaction.</p>

<p><a name="Asymetric Telecollaboration in Virtual Reality"> </a></p>

<h4 id="asymetric-telecollaboration-in-virtual-reality">Asymetric Telecollaboration in Virtual Reality</h4>

<p><em>Thibault Porssut and Jean-Rémy Chardonnet</em></p>

<p>Abstract: We present a first study where we combine two asymetric virtual reality systems for telecollaboration purposes: a CAVE system and a head-mounted display (HMD), using a server-client type architecture. Experiments on a puzzle game in limited time, alone and in collaboration, show that combining asymetric systems reduces cognitive load. Moreover, the participants reported preferring working in collaboration and showed to be more efficient in collaboration. These results provide insights in combining several low cost HMDs with a unique expensive CAVE.</p>

<p><a name="An Immersive Approach to Visualizing Perceptual Disturbances"> </a></p>

<h4 id="an-immersive-approach-to-visualizing-perceptual-disturbances">An Immersive Approach to Visualizing Perceptual Disturbances</h4>

<p><em>Grace M. Rodriguez, Marvis Cruz, Andrew Solis, Patricia Ordóñez, and Brian C. McCann</em></p>

<p>Abstract: Through their experience with the ICERT REU program at the Texas Advanced Computing Center (TACC), two undergraduate students from the University of Puerto Rico and the University of Florida have initiated a collaboration between their home institutions and TACC exploring the possibility of using immersion to simulate perceptual disturbances. Perceptual disturbances are subjective in nature, and difficult to communicate verbally. Often caretakers or those closest to sufferers have difficulty understanding the nature of their suffering. Immersion provides an exciting opportunity to directly communicate percepts with clinicians and loved ones. Here, we present a prototype environment meant to simulate some of the perceptual disturbances associated with seizures and epilepsy. Following further validation of our approach, we hope to promote awareness and empathy for these often jarring phenomena.</p>

<p><a name="Corrective Feedback for Depth Perception in CAVE-Like Systems"> </a></p>

<h4 id="corrective-feedback-for-depth-perception-in-cave-like-systems">Corrective Feedback for Depth Perception in CAVE-Like Systems</h4>

<p><em>Adrian K. T. Ng, Leith K. Y. Chan, and Henry Y. K. Lau</em></p>

<p>Abstract: The perceived distance estimation in an immersive virtual reality system is generally underestimated to the actual distance. Approaches had been found to provide users with better dimensional perception. One method used in head-mounted displays is to interact by walking with visual feedback, but it is not suitable for a CAVE-like system, like imseCAVE with confined spaces for walking. A verbal corrective feedback mechanism is proposed. The result shows that estimation accuracy generally improves after eight feedback trials although some estimations become overestimated. One possible explanation is the need of more verbal feedback trials. Further research on top-down approach for improvement in depth perception is suggested.</p>

<p><a name="Measurement of 3D-Velocity by High-Frame-Rate Optical Mouse Sensors to Extrapolate 3D Position Captured by a Low-Frame-Rate Stereo Camera"> </a></p>

<h4 id="measurement-of-3d-velocity-by-high-frame-rate-optical-mouse-sensors-to-extrapolate-3d-position-captured-by-a-low-frame-rate-stereo-camera">Measurement of 3D-Velocity by High-Frame-Rate Optical Mouse Sensors to Extrapolate 3D Position Captured by a Low-Frame-Rate Stereo Camera</h4>

<p><em>Itsuo Kumazawa, Toshihiro Kai, Yoshikazu Onuki, and Shunsuke Ono</em></p>

<p>Abstract: The frame rate of existing stereo cameras is not enough to track quick hand or finger actions. It also requires lots of computational cost to find correspondence between stereo images to compute distance. The recently commercialized 3D position sensors such as TOF cameras or Leap Motion needs strong illumination to ensure sufficient optical energy for the high frame rate sensing. To overcome these problems, this paper proposes to use a pair of optical-mouse-sensors as a stereo image sensor to measure 3D-velocity and use it to extrapolate 3D position measured by a low-frame-rate stereo camera. It is shown that quick hand actions are tracked under ordinary in-door lighting condition. As 2D velocities are computed inside the optical-mouse-sensors, computation and communication costs are drastically reduced.</p>

<p><a name="Using Augmented Reality to Improve Dismounted Operators' Situation Awareness"> </a></p>

<h4 id="using-augmented-reality-to-improve-dismounted-operators-situation-awareness">Using Augmented Reality to Improve Dismounted Operators’ Situation Awareness</h4>

<p><em>William Losina Brandão and Márcio Sarroglia Pinho</em></p>

<p>Abstract: Whether it in the military, law enforcement or private security, dismounted operators tend to deal with a large amount of volatile information that may or may not be relevant according to a variety of factors. In this paper we draft some ideas on the building blocks of an augmented reality system aimed to improve the situational awareness of dismounted operators by filtering, organizing, and displaying this information in a way that reduces the strain over the operator.</p>

<p><a name="Gauntlet: Travel Technique for Immersive Environments using Non-dominant Hand"> </a></p>

<h4 id="gauntlet-travel-technique-for-immersive-environments-using-non-dominant-hand">Gauntlet: Travel Technique for Immersive Environments using Non-dominant Hand</h4>

<p><em>Mathew Tomberlin, Liudmila Tahai, and Krzysztof Pietroszek</em></p>

<p>Abstract: We present Gauntlet, a travel technique for immersive environments that uses non-dominant hand tracking and a fist gesture to translate and rotate the viewport. The technique allows for simultaneous use of the dominant hand for other spatial input tasks. Applications of Gauntlet include FPS games, and other application domains where navigation should be performed together with other tasks. We release the technique along with an example application, a VR horror game, as an open source project.</p>

<p><a name="Peers at Work: Economic Real-Effort Experiments in the Presence of Virtual Co-workers"> </a></p>

<h4 id="peers-at-work-economic-real-effort-experiments-in-the-presence-of-virtual-co-workers">Peers at Work: Economic Real-Effort Experiments in the Presence of Virtual Co-workers</h4>

<p><em>Andrea Bönsch, Jonathan Wendt, Heiko Overath, Özgür Gürerk, Christine Harbring, Christian Grund, Thomas Kittsteiner, and Torsten W. Kuhlen</em></p>

<p>Abstract: Traditionally, experimental economics uses controlled and incentivized field and lab experiments to analyze economic behavior. However, investigating peer effects in the classic settings is challenging due to the reflection problem: Who is influencing whom?  To overcome this, we enlarge the methodological toolbox of these experiments by means of Virtual Reality. After introducing and validating a real-effort sorting task, we embed a virtual agent as peer of a human subject, who independently performs an identical sorting task. We conducted two experiments investigating (a) the subject’s productivity adjustment due to peer effects and (b) the incentive effects on competition. Our results indicate a great potential for Virtual-Reality-based economic experiments.</p>

<p><a name="Efficient Sound Synthesis for Natural Scenes"> </a></p>

<h4 id="efficient-sound-synthesis-for-natural-scenes">Efficient Sound Synthesis for Natural Scenes</h4>

<p><em>Kai Wang, Haonan Cheng, and Shiguang Liu</em></p>

<p>Abstract: This paper presents a novel framework to generate the sound of outdoor natural scenes, such as waterfall, ocean, etc. Our method firstly simulates liquid with a grid-based method. Then combined with the movement of liquid, we generate seed-particles which represent bubbles, foams or splashes. Next, we assign each seed-particles a radius with a new radius distribution model. By calculating the bubbles’ pressure wave we generate the sound. Experiments demonstrated that our novel framework can efficiently synthesize the sounds for natural scenes.</p>

<p><a name="Poster Session B"> </a></p>

<h2 id="poster-session-b">Poster Session B</h2>

<h4 id="tuesday-march-21">Tuesday, March 21</h4>

<p><a name="A Diminished Reality Simulation for Driver-Car Interaction with Transparent Cockpits"> </a></p>

<h4 id="a-diminished-reality-simulation-for-driver-car-interaction-with-transparent-cockpits">A Diminished Reality Simulation for Driver-Car Interaction with Transparent Cockpits</h4>

<p><em>Patrick Lindemann and Gerhard Rigoll</em></p>

<p>Abstract: We anticipate advancements in mixed reality device technology which might benefit driver-car interaction scenarios and present a simulated diminished reality interface for car drivers. It runs in a custom driving simulation and allows drivers to perceive otherwise occluded objects of the environment through the car body. We expect to obtain insights that will be relevant to future real-world applications. We conducted a pre-study with participants performing a driving task with the prototype in a CAVE-like virtual environment. Users preferred large-sized see-through areas over small ones but had differing opinions on the level of transparency to use. In future work, we plan additional evaluations of the driving performance and will further extend the simulation.</p>

<p><a name="Immersive and Collaborative Taichi Motion Learning in Various VR Environments"> </a></p>

<h4 id="immersive-and-collaborative-taichi-motion-learning-in-various-vr-environments">Immersive and Collaborative Taichi Motion Learning in Various VR Environments</h4>

<p><em>Tianyu He, Xiaoming Chen, Zhibo Chen, Ye Li, Sen Liu, Junhui Hou, and Ying He</em></p>

<p>Abstract: Learning “motion” online or from video tutorials is usually inefficient since it is difficult to deliver “motion” information in traditional ways and in the ordinary PC platform. This paper presents ImmerTai, a system that can efficiently teach motion, in particular Chinese Taichi motion, in various immersive environments. ImmerTai captures the Taichi expert’s motion and delivers to students the captured motion in multi-modal forms in immersive CAVE, HMD as well as ordinary PC environments. The students’ motions are captured too for quality assessment and utilized to form a virtual collaborative learning atmosphere. We built up a Taichi motion dataset with 150 fundamental Taichi motions captured from 30 students, on which we evaluated the learning effectiveness and user experience of ImmerTai. The results show that ImmerTai can enhance the learning efficiency by up to 17.4% and the learning quality by up to 32.3%.</p>

<p><a name="Virtual Zero Gravity Impact on Internal Gravity Model"> </a></p>

<h4 id="virtual-zero-gravity-impact-on-internal-gravity-model">Virtual Zero Gravity Impact on Internal Gravity Model</h4>

<p><em>Thibault Porssut, Henrique G. Debarba, Elisa Canzoneri, Bruno Herbelin, and Ronan Boulic</em></p>

<p>Abstract: This project investigates the impact of a virtual zero gravity experience on the human gravity model. In the planned experiment, subjects are immersed with HMD and full body motion capture in a virtual world exhibiting either normal gravity or the apparent absence of gravity (i.e. body and objects floating in space). The study evaluates changes in the subjects’ gravity model by observing changes on motor planning of actions dependent on gravity. Our goal is to demonstrate that a virtual reality exposure can induce some modifications to the humans internal gravity model, even if users remain under normal gravity condition in reality.</p>

<p><a name="Approximating Optimal Sets of Views in Virtual Scenes"> </a></p>

<h4 id="approximating-optimal-sets-of-views-in-virtual-scenes">Approximating Optimal Sets of Views in Virtual Scenes</h4>

<p><em>Sebastian Freitag, Clemens Löbbert, Benjamin Weyers, and Torsten W. Kuhlen</em></p>

<p>Abstract: Viewpoint quality estimation methods allow the determination of the most informative position in a scene. However, a single position usually cannot represent an entire scene, requiring instead a set of several viewpoints. Measuring the quality of such a set of views, however, is not trivial, and the computation of an optimal set of views is an NP-hard problem. Therefore, in this work, we propose three methods to estimate the quality of a set of views. Furthermore, we evaluate three approaches for computing an approximation to the optimal set (two of them new) regarding effectiveness and efficiency.</p>

<p><a name="Estimating the Motion-to-Photon Latency in Head Mounted Displays"> </a></p>

<h4 id="estimating-the-motion-to-photon-latency-in-head-mounted-displays">Estimating the Motion-to-Photon Latency in Head Mounted Displays</h4>

<p><em>Jingbo Zhao, Robert S. Allison, Margarita Vinnikov, and Sion Jennings</em></p>

<p>Abstract: We present a method for estimating the Motion-to-Photon (End-to-End) latency of head mounted displays (HMDs). The specific HMD evaluated in our study was the Oculus Rift DK2, but the procedure is general. We mounted the HMD on a pendulum to introduce damped sinusoidal motion to the HMD during the pendulum swing. The latency was estimated by calculating the phase shift between the captured signals of the physical motion of the HMD and a motion-dependent gradient stimulus rendered on the display. We used the proposed method to estimate both rotational and translational Motion-to-Photon latencies of the Oculus Rift DK2.</p>

<p><a name="Object Location Memory Error in Virtual and Real Environments"> </a></p>

<h4 id="object-location-memory-error-in-virtual-and-real-environments">Object Location Memory Error in Virtual and Real Environments</h4>

<p><em>Mengxin Xu, María Murcia-López, and Anthony Steed</em></p>

<p>Abstract: We aim to further explore the transfer of spatial knowledge from virtual to real spaces. Based on previous research on spatial memory in immersive virtual reality (VR) we ran a study that looked at the effect of three locomotion techniques (joystick, pointing-and-teleporting and walking-in-place) on object location learning and recall. Participants were asked to learn the location of a virtual object in a virtual environment (VE). After a short period of time they were asked to recall the location by placing a real version of the object in the real-world equivalent environment. Results indicate that the average placement error, or distance between original and recalled object location, is approximately 20cm for all locomotion technique conditions. This result is similar to the outcome of a previous study on spatial memory in VEs that used real walking. We report this unexpected finding and suggest further work on spatial memory in VR by recommending the replication of this study in different environments and using objects with a wider diversity of properties, including varying sizes and shapes.</p>

<p><a name="Tactile Feedback Enhanced with Discharged Elastic Energy and Its Effectiveness for In-Air Key-Press and Swipe Operations"> </a></p>

<h4 id="tactile-feedback-enhanced-with-discharged-elastic-energy-and-its-effectiveness-for-in-air-key-press-and-swipe-operations">Tactile Feedback Enhanced with Discharged Elastic Energy and Its Effectiveness for In-Air Key-Press and Swipe Operations</h4>

<p><em>Itsuo Kumazawa, Souma Suzuki, Yoshikazu Onuki, and Shunsuke Ono</em></p>

<p>Abstract: This paper presents a simple but effective way of enhancing tactile stimulus by a mechanism with springs to preserve elastic energies charged in a prior energy-charging phase and discharge them to enhance the force to hit a finger in the stimulating phase. With this mechanism, a small and light stimulator attached to the fingertip is developed and demonstrated to generate the tactile feedback strong enough to make people feel as if their fingers collide with a virtual object. It is also shown that the durations of the two phases can be as short as a few milliseconds so that the latency in tactile feedback can be negligible. The performance of the mechanism and the effectiveness of its tactile feedback are evaluated for in-air key-press and swipe operations.</p>

<p><a name="BlowClick 2.0: A Trigger Based on Non-verbal Vocal Input"> </a></p>

<h4 id="blowclick-20-a-trigger-based-on-non-verbal-vocal-input">BlowClick 2.0: A Trigger Based on Non-verbal Vocal Input</h4>

<p><em>Daniel Zielasko, Neha Neha, Benjamin Weyers, and Torsten W. Kuhlen</em></p>

<p>Abstract: The use of non-verbal vocal input (NVVI) as a hand-free trigger approach has proven to be valuable in previous work [Zielasko2015]. Nevertheless, BlowClick's original detection method is vulnerable to false positives and, thus, is limited in its potential use, e.g., together with acoustic feedback for the trigger. Therefore, we extend the existing approach by adding common machine learning methods. We found that a support vector machine (SVM) with Gaussian kernel performs best for detecting blowing with at least the same latency and more precision as before. Furthermore, we added acoustic feedback to the NVVI trigger, which increases the user's confidence. To evaluate the advanced trigger technique, we conducted a user study (n=33). The results confirm that it is a reliable trigger; alone and as part of a hands-free point-and-click interface.</p>

<p><a name="KKse: Safety Education System of the Child in the Kitchen Knife Cooking"> </a></p>

<h4 id="kkse-safety-education-system-of-the-child-in-the-kitchen-knife-cooking">KKse: Safety Education System of the Child in the Kitchen Knife Cooking</h4>

<p><em>Shiho Saito, Koichi Hirota, and Takuya Nojima</em></p>

<p>Abstract: The Kitchen Knife Safety Educator (KKse) is a safety education system designed to teach children how to correctly use cooking knives. Cooking is important for children to learn about what they eat. In addition, that is also important for daily communication between children and their parents. However, it is dangerous for young children to handle cooking knives. Because of this danger, parents often try to keep their young children away from the kitchen. Our proposed system will contribute to not only improving children’s cooking skills, but also improving communication between parents and children. The system composed of a virtual knife with haptic feedback function, a touch/force sensitive virtual food and a two-dimensional force sensitive cutting board. This system was developed to teach a fundamental cutting method, the “thrusting cut”. This paper describes the detail of the system.</p>

<p><a name="Air Cushion: A Pilot Study of the Passive Technique to Mitigate Simulator Sickness by Responding to Vection"> </a></p>

<h4 id="air-cushion-a-pilot-study-of-the-passive-technique-to-mitigate-simulator-sickness-by-responding-to-vection">Air Cushion: A Pilot Study of the Passive Technique to Mitigate Simulator Sickness by Responding to Vection</h4>

<p><em>Yoshikazu Onuki, Shunsuke Ono, and Itsuo Kumazawa</em></p>

<p>Abstract: Simulator sickness is an issue in virtual reality environments. In a virtual world, sensory conflict between visual sensation and self-motion perception occurs readily. Contradiction between visual and vestibular sensation is a dominant cause of motion sickness. Vection is a visually evoked illusion of self-motion. Vection occurs when a stationary human experiences locomotor stimulation over a wider area of the field of view, and senses motion when in fact there is none. Strong vection has been associated with simulator sickness. In this poster, the authors present results of a pilot study based on a hypothesis that simulator sickness can be mitigated by passively responding to the body sway. Commercially available air cushions were applied for VR environments. Measurable mitigation of simulator sickness was achieved by physically responding to vection. Allowing body sway encourages moder-ating the sensory conflict between visual sensation and self-motion perception. Also, the shapes of air cushions on seat backs were found to be an important variable.</p>

<p><a name="A Haptic Three-Dimensional Shape Display with Three Fingers Grasping"> </a></p>

<h4 id="a-haptic-three-dimensional-shape-display-with-three-fingers-grasping">A Haptic Three-Dimensional Shape Display with Three Fingers Grasping</h4>

<p><em>Takuya Handa, Kenji Murase, Makiko Azuma, Toshihiro Shimizu, Satoru Kondo, and Hiroyuki Shinoda</em></p>

<p>Abstract: The main goal of our research is to develop a haptic display that makes it possible to convey shapes, hardness, and textures of objects displayed on 3D TV. Our evolved device has three 5 mm diameter actuating spheres arranged in triangular geometry on each of three fingertips (thumb, index finger, middle finger). In this paper, we describe an overview of a novel haptic device and the first experimental results that twelve subjects had succeeded to recognize the size of cylinders and side geometry of a cuboid and a hexagonal prism.</p>

<p><a name="Data Fragment: Virtual Reality for Viewing and Querying Large Image Sets"> </a></p>

<h4 id="data-fragment-virtual-reality-for-viewing-and-querying-large-image-sets">Data Fragment: Virtual Reality for Viewing and Querying Large Image Sets</h4>

<p><em>Theophilus Teo, Mitchell Norman, Matt Adcock, and Bruce H. Thomas</em></p>

<p>Abstract: This paper presents our new Virtual Reality (VR) interactive visualization techniques to assist users querying large image sets. The VR system allows users to query a set of images on four different filters such as locations and keywords. The goal is to investigate if a VR platform is preferred over a non-VR platform for viewing and querying large image sets. We employed an HTC Vive and a traditional desktop screen to represent VR and non-VR platforms. We found users preferred the VR platform over the traditional desktop screen.</p>

<p><a name="Towards a Design Space Characterizing Workflows That Take Advantage of Immersive Visualization"> </a></p>

<h4 id="towards-a-design-space-characterizing-workflows-that-take-advantage-of-immersive-visualization">Towards a Design Space Characterizing Workflows That Take Advantage of Immersive Visualization</h4>

<p><em>Tom Vierjahn, Daniel Zielasko, Kees van Kooten, Peter Messmer, Bernd Hentschel, Torsten W. Kuhlen, and Benjamin Weyers</em></p>

<p>Abstract: Immersive visualization (IV) fosters the creation of mental images of a data set, a scene, a procedure, etc. We devise an initial version of a design space for categorizing workflows that take advantage of IV. From this categorization, specific requirements for an actual, seamless IV-integration can be derived. We validate the design space with three workflows investigated in our research projects.</p>

<p><a name="Hand Gesture Controls for Image Categorization in Immersive Virtual Environments"> </a></p>

<h4 id="hand-gesture-controls-for-image-categorization-in-immersive-virtual-environments">Hand Gesture Controls for Image Categorization in Immersive Virtual Environments</h4>

<p><em>Chao Peng, Jeffrey T. Hansberger, Lizhou Cao, and Vaidyanath Areyur Shanthakumar</em></p>

<p>Abstract: In a situation where a large and chaotic collection of digital images must be manually sorted or categorized, there are two challenges: (1) unnatural actions during a prolonged human-computer interaction and (2) limited display space for image browsing. An immersive 3D interface is prototyped, where a person sorts a large collection of digital images with his or her bare hands in a virtual environment, and performs hand motions matching characteristics of sorting gestures in the real world. The virtual reality environment provides extra levels of immersion for displaying images.</p>

<p><a name="Mixed Reality Training for Tank Platoon Leader Communication Skills"> </a></p>

<h4 id="mixed-reality-training-for-tank-platoon-leader-communication-skills">Mixed Reality Training for Tank Platoon Leader Communication Skills</h4>

<p><em>Peter Khooshabeh, Igor Choromanski, Catherine Neubauer, David M. Krum, Ryan Spicer, and Julia Campbell</em></p>

<p>Abstract: Here we describe the design and usability evaluation of a mixed reality prototype to simulate the role of a tank platoon leader, who is an individual who not only is a tank commander, but also directs a platoon of three other tanks with their own respective tank commanders. The domain of tank commander training has relied on physical simulators of the actual Abrams tank and encapsulates the whole crew. The TALK-ON system we describe here focuses on training communication skills of the leader in a simulated tank crew. We report results from a usability evaluation and discuss how they will inform our future work for collective tank training.</p>

<p><a name="Prioritization and Static Error Compensation for Multi-camera Collaborative Tracking in Augmented Reality"> </a></p>

<h4 id="prioritization-and-static-error-compensation-for-multi-camera-collaborative-tracking-in-augmented-reality">Prioritization and Static Error Compensation for Multi-camera Collaborative Tracking in Augmented Reality</h4>

<p><em>Jianren Wang, Long Qian, Ehsan Azimi, and Peter Kazanzides</em></p>

<p>Abstract: An effective and simple method is proposed for multi-camera collaborative tracking, based on the prioritization of all tracking units, and then modeling the discrepancy between different tracking units as a locally static transformation error. Static error compensation is applied to the lower-priority tracking systems when high-priority trackers are not available. The method does not require high-end or carefully calibrated tracking units, and is able to effectively provide a comfortable augmented reality experience for users. A pilot study demonstrates the validity of the proposed method.</p>

<p><a name="Resolution-Defined Projections for Virtual Reality Video Compression"> </a></p>

<h4 id="resolution-defined-projections-for-virtual-reality-video-compression">Resolution-Defined Projections for Virtual Reality Video Compression</h4>

<p><em>Charles Dunn and Brian Knott</em></p>

<p>Abstract: Spherical data compression methods for Virtual Reality (VR) currently leverage popular rectangular data encoding algorithms. Traditional compression algorithms have massive adoption and hardware support on computers and mobile devices. Efficiently utilizing these two-dimensional compression methods for spherical data necessitates a projection from the three-dimensional surface of a sphere to a two-dimensional rectangle. Any such projection affects the final resolution distribution of the data after decoding. Popular projections used for VR video benefit from mathematical or geometric simplicity, but result in suboptimal resolution distributions. We introduce a method for generating a projection to match a desired resolution function. This method allows for customized projections with smooth, continuous and optimal resolution functions. Compared to commonly used projections, our resolution-defined projections drastically improve compression ratios for any given quality.</p>

<p><a name="The Effect of Geometric Realism on Presence in a Virtual Reality Game"> </a></p>

<h4 id="the-effect-of-geometric-realism-on-presence-in-a-virtual-reality-game">The Effect of Geometric Realism on Presence in a Virtual Reality Game</h4>

<p><em>Jonatan S. Hvass, Oliver Larsen, Kasper B. Vendelbo, Niels C. Nilsson, Rolf Nordahl, and Stefania Serafin</em></p>

<p>Abstract: Previous research on visual realism and presence has not involved scenarios, graphics, and hardware representative of commercially available VR games. This poster details a between-subjects study (n=50) exploring if polygon count and texture resolution influence presence during exposure to a VR game. The results suggest that a higher polygon count and texture resolution increased presence as assessed by means of self-reports and physiological measures.</p>

<p><a name="An Exploration of Input Conditions for Virtual Teleportation"> </a></p>

<h4 id="an-exploration-of-input-conditions-for-virtual-teleportation">An Exploration of Input Conditions for Virtual Teleportation</h4>

<p><em>Emil R. Høeg, Kevin V. Ruder, Niels C. Nilsson, Rolf Nordahl, and Stefania Serafin</em></p>

<p>Abstract: This poster describes a within-groups study (n=17) comparing participants' experience of three different input conditions for instigating virtual teleportation (button clicking, physical jumping, and fist clenching). The results indicated that teleportation by clicking a button generally required less explicit attention and was perceived as more enjoyable, less disorienting, and less physically demanding.</p>

<p><a name="A Preliminary Study of Users' Experiences of Meditation in Virtual Reality"> </a></p>

<h4 id="a-preliminary-study-of-users-experiences-of-meditation-in-virtual-reality">A Preliminary Study of Users’ Experiences of Meditation in Virtual Reality</h4>

<p><em>Thea Andersen, Gintare Anisimovaite, Anders Christiansen, Mohamed Hussein, Carol Lund, Thomas Nielsen, Eoin Rafferty, Niels C. Nilsson, Rolf Nordahl, and Stefania Serafin</em></p>

<p>Abstract: This poster describes a between-groups study (n=24) exploring the use of virtual reality (VR) for facilitating focused meditation. Half of the participants were exposed to a meditation session combing the sound of a guiding voice and a visual environment including virtual objects for the participants to focus on. The other half of the participants was only exposed to the auditory guide. The participants' experience of the sessions was assessed using self-reported measures of perceived concentration, temporal duration, stress reduction, and comfort. Interestingly, no statistically significant differences were found between the two conditions. This finding may be revealing in regards to the usefulness of VR-based meditation.</p>

<p><a name="Observation of Mirror Reflection and Voluntary Self-Touch Enhance Self-Recognition for a Telexistence Robot"> </a></p>

<h4 id="observation-of-mirror-reflection-and-voluntary-self-touch-enhance-self-recognition-for-a-telexistence-robot">Observation of Mirror Reflection and Voluntary Self-Touch Enhance Self-Recognition for a Telexistence Robot</h4>

<p><em>Yasuyuki Inoue, Fumihiro Kato, MHD Yamen Saraiji, Charith Lasantha Fernando, and Susumu Tachi</em></p>

<p>Abstract: In this paper, we analyze the subjective feelings about the body of the operator of a telexistence system. We investigate whether a mirror reflection and self-touch affect body ownership and agency for a surrogate robot avatar in a virtual reality experiment. Results showed that the presence of tactile sensations synchronized with the view of self-touch events enhanced mirror self-recognition.</p>

<p><a name="Adaptive 360-Degree Video Streaming using Layered Video Coding"> </a></p>

<h4 id="adaptive-360-degree-video-streaming-using-layered-video-coding">Adaptive 360-Degree Video Streaming using Layered Video Coding</h4>

<p><em>Afshin Taghavi Nasrabadi, Anahita Mahzari, Joseph D. Beshay, and Ravi Prakash</em></p>

<p>Abstract: Virtual reality and 360-degree video streaming are growing rapidly; however, streaming 360-degree video is very challenging due to high bandwidth requirements. To address this problem, the video quality is adjusted according to the user viewport prediction. High quality video is only streamed for the user viewport, reducing the overall bandwidth consumption. Existing solutions use shallow buffers limited by the accuracy of viewport prediction. Therefore, playback is prone to video freezes which are very destructive for the Quality of Experience(QoE). We propose using layered encoding for 360-degree video to improve QoE by reducing the probability of video freezes and the latency of response to the user head movements. Moreover, this scheme reduces the storage requirements significantly and improves in-network cache performance.</p>

<p><a name="A Mixed Reality Tele-presence Platform to Exchange Emotion and Sensory Information Based on MPEG-V Standard"> </a></p>

<h4 id="a-mixed-reality-tele-presence-platform-to-exchange-emotion-and-sensory-information-based-on-mpeg-v-standard">A Mixed Reality Tele-presence Platform to Exchange Emotion and Sensory Information Based on MPEG-V Standard</h4>

<p><em>Hojun Lee, Gyutae Ha, Sangho Lee, and Shiho Kim</em></p>

<p>Abstract: We have implemented a mixed reality telepresence platform providing a user experience (UX) of exchanging emotional expressions as well as information among a group of participants. The implemented system provides a platform to experience an immersive live scene through a Head-Mounted Display (HMD) and sensory information to a VR HMD user at a remote place. Moreover, the user at a remote place can share and exchange emotional expressions with other users at another remote location by using 360° cameras, environmental sensors compliant with MPEG-V, and a game cloud server combined with a technique of holographic display. We demonstrated that emotional expressions of an HMD worn participant were shared with a group of other participants in the remote place while watching a sports game on a big screen TV.</p>

<p><a name="Evaluation of Labelling Layout Methods in Augmented Reality"> </a></p>

<h4 id="evaluation-of-labelling-layout-methods-in-augmented-reality">Evaluation of Labelling Layout Methods in Augmented Reality</h4>

<p><em>Gang Li, Yue Liu, and Yongtian Wang</em></p>

<p>Abstract: View management techniques are commonly used for labelling of objects in augmented reality environments. Combining with image analysis, search space and adaptive representations, they can be utilized to achieve desired labelling tasks. However, the evaluation of different search space methods on labelling are still an open problem. In this paper, we propose an image analysis based view management method, which first adopts the image processing to superimpose 2D labels to the specific object. We then conduct three search space methods to an augmented reality scenario. Without the requirements of setting rules and constraints for occlusion among the labels, the results of three search space methods are evaluated by using objective analysis of related parameters. The evaluation results indicate that different search space methods could generate different time costs and occlusion, thereby affecting the final labelling effects.</p>

<p><a name="Real-Time Interactive AR System for Broadcasting"> </a></p>

<h4 id="real-time-interactive-ar-system-for-broadcasting">Real-Time Interactive AR System for Broadcasting</h4>

<p><em>Hyunwoo Cho, Sung-Uk Jung, and Hyung-Keun Jee</em></p>

<p>Abstract: For live television broadcast such as the educational program for children conducted through viewer participation, the smooth integration of virtual contents and the interaction between the casts and them are quite important issues. Recently there have been many attempts to make aggressive use of interactive virtual contents in live broadcast due to the advancement of AR/VR technology and virtual studio technology. These previous works have many limitations that do not support real-time 3D space recognition or immersive interaction. In this sense, we propose an augmented reality based real-time broadcasting system which perceives the indoor space using a broadcasting camera and a RGB-D camera. Also, the system can support the real-time interaction between the augmented virtual contents and the casts. The contribution of this work is the development of a new augmented reality based broadcasting system that not only enables filming using compatible interactive 3D contents in live broadcast but also drastically reduces the production costs. For the practical use, the proposed system was demonstrated in the actual broadcast program called “Ding Dong Dang Kindergarten” which is a representative children educational program on the national broadcasting channel of Korea.</p>

<p><a name="Texturing of Augmented Reality Character Based on Colored Drawing"> </a></p>

<h4 id="texturing-of-augmented-reality-character-based-on-colored-drawing">Texturing of Augmented Reality Character Based on Colored Drawing</h4>

<p><em>Hengheng Zhao, Ping Huang, and Junfeng Yao</em></p>

<p>Abstract: Coloring book can inspire imaginary and creativity of children. However, with the rapid development of digital devices and internet, traditional coloring book tends to be not attractive for children any more. Thus, we propose an idea of applying augmented reality technology to traditional coloring book. After children finish coloring characters in the printed coloring book, they can inspect their work using a mobile device. The drawing is detected and tracked so that the video stream is augmented with a 3D character textured according to their coloring. This is possible thanks to several novel technical contributions. We present a texture process that generates texture map for 3D augmented reality character from 2D colored drawing using a lookup map. Considering the movement of the mobile device and drawing, we give an efficient method to track the drawing surface.</p>

<p><a name="VROnSite: Towards Immersive Training of First Responder Squad Leaders in Untethered Virtual Reality"> </a></p>

<h4 id="vronsite-towards-immersive-training-of-first-responder-squad-leaders-in-untethered-virtual-reality">VROnSite: Towards Immersive Training of First Responder Squad Leaders in Untethered Virtual Reality</h4>

<p><em>Annette Mossel, Mario Froeschl, Christian Schoenauer, Andreas Peer, Johannes Goellner, and Hannes Kaufmann</em></p>

<p>Abstract: We present the VROnSite platform that enables immersive training of first responder on-site squad leaders. Our training platform is fully immersive, entirely untethered to ease use and provides two means of navigation - abstract and natural walking - to simulate stress and exhaustion, two important factors for decision making. With the platform's capabilities, we close a gap in prior art for first responder training. Our research is closely interlocked with stakeholders from fire brigades and paramedics to gather early feedback in an iterative design process. In this paper, we present our first research results, which are the system's design rationale, the single user training prototype and results from a preliminary user study.</p>

<p><a name="Recommender System for Physical Object Substitution in VR"> </a></p>

<h4 id="recommender-system-for-physical-object-substitution-in-vr">Recommender System for Physical Object Substitution in VR</h4>

<p><em>Jose Garcia Estrada and Adalberto L. Simeone</em></p>

<p>Abstract: This poster introduces the development of a recommender system to guide users in adapting the Virtual Environment into matching objects in the physical world. Emphasis is placed on avoiding cognitive overload resulting from providing options for substitution without considering the number of physical objects present. This is the first step towards a comprehensive recommender system for user-driven adaptation of Virtual Environments through immersive Virtual Reality systems.</p>

<p><a name="Itapeva 3D: Being Indiana Jones in Virtual Reality"> </a></p>

<h4 id="itapeva-3d-being-indiana-jones-in-virtual-reality">Itapeva 3D: Being Indiana Jones in Virtual Reality</h4>

<p><em>Eduardo Zilles Borba, Andre Montes, Roseli de Deus Lopes, Marcelo Knorich Zuffo, and Regis Kopper</em></p>

<p>Abstract: This poster presents the conceptual process of developing Itapeva 3D, a Virtual Reality (VR) archeology experience. It describes the technical spectrum of cyber-archeology process applied to the creation of a fully immersive and interactive virtual environment (VE), which represents Itapeva Rocky Shelter, a prehistoric archeological site in Brazil. The workflow starts with a real world data capture – laser scanners, drones and photogrammetry, continues with the transposition of the captured information into a 3D surface model capable of real-time rendering to head-mounted displays (HMDs), and ends with the design of interactive features allowing users to experience the virtual archeological site. The main objective of this VR model is to make plausible to general public to feel what it means to explore an otherwise restricted and ephemeral place. As final thoughts it is reported on preliminary results from an initial user observation.</p>

<p><a name="Sound Design in Virtual Reality Concert Experiences using a Wave Field Synthesis Approach"> </a></p>

<h4 id="sound-design-in-virtual-reality-concert-experiences-using-a-wave-field-synthesis-approach">Sound Design in Virtual Reality Concert Experiences using a Wave Field Synthesis Approach</h4>

<p><em>Rasmus B. Lind, Victor Milesen, Dina M. Smed, Simone P. Vinkel, Francesco Grani, Niels C. Nilsson, Lars Reng, Rolf Nordahl, and Stefania Serafin</em></p>

<p>Abstract: In this paper we propose an experiment that evaluates the influence of audience noise on the feeling of presence and the perceived qual- ity in a virtual reality concert experience delivered using Wave Field Synthesis. A 360 degree video of a live rock concert from a local band was recorded. Single sound sources from the stage and the PA system were recorded, as well as the audience noise, and impulse responses of the concert venue. The audience noise was imple- mented in the production phase. A comparative study compared an experience with and without audience noise. In a between sub- ject experiment with 30 participants we found that audience noise does not have a significant impact on presence. However, qualita- tive evaluations show that the naturalness of the sonic experience delivered through wavefield synthesis had a positive impact on the participants.</p>

<p><a name="Effect on High versus Low Fidelity Haptic Feedback in a Virtual Reality Baseball Simulation"> </a></p>

<h4 id="effect-on-high-versus-low-fidelity-haptic-feedback-in-a-virtual-reality-baseball-simulation">Effect on High versus Low Fidelity Haptic Feedback in a Virtual Reality Baseball Simulation</h4>

<p><em>Andreas Ryge, Lui Thomsen, Theis Berthelsen, Jonatan S. Hvass, Lars Koreska, Casper Vollmers, Niels C. Nilsson, Rolf Nordahl, and Stefania Serafin</em></p>

<p>Abstract: In this paper we present a within-subjects study (n=26) comparing participants’ experience of three kinds of haptic feedback (no haptic feedback, low fidelity haptic feedback and high fidelity haptic feed- back) simulating the impact between a virtual baseball bat and ball. We noticed some minor effect on high fidelity versus low fidelity haptic feedback, but haptic feedback generally enhanced realism and quality of experience.</p>

<p><a name="Immerj: A Novel System for Democratizing Immersive Storytelling"> </a></p>

<h4 id="immerj-a-novel-system-for-democratizing-immersive-storytelling">Immerj: A Novel System for Democratizing Immersive Storytelling</h4>

<p><em>Sarang S. Bhadsavle, Xie Hunt Shannon Yap, Justin Segler, Rahul Jaisimha, Nishant Raman, Yengzhou Feng, Sierra J. Biggs, Micah Peoples, Robert B. Brenner, and Brian C. McCann</em></p>

<p>Abstract: Immersive technologies such as 360◦ cameras and head-mounted displays (HMDs) have become affordable to the average consumer, opening up new audiences for storytellers. However, existing immersive post-production software often requires too great a technical or financial investment for smaller creative shops, including most of the country’s newspapers and local broadcast journalism organizations. Game engines, for example, are unnecessarily complicated for simple 360◦ video projects. Introducing Immerj - an open source abstraction layer simplifying the Unity3D game engine’s interface for immersive content creators. Our primary collaborator, Professor R.B. Brenner, director of the School of Journalism at the University of Texas at Austin, organized hands-on demos with our team and journalists and designers from some of the top news organizations all over the country in order to follow a human-centered design process. In just over one year, a small team of undergraduate researchers at the Texas Advanced Computing Center (TACC) has created a potentially disruptive democratization of technology.</p>

<p><a name="Assisted Travel Based on Common Visibility and Navigation Meshes"> </a></p>

<h4 id="assisted-travel-based-on-common-visibility-and-navigation-meshes">Assisted Travel Based on Common Visibility and Navigation Meshes</h4>

<p><em>Sebastian Freitag, Benjamin Weyers, and Torsten W. Kuhlen</em></p>

<p>Abstract: The manual adjustment of travel speed to cover medium or large distances in virtual environments may increase cognitive load, and manual travel at high speeds can lead to cybersickness due to inaccurate steering. In this work, we present an approach to quickly pass regions where the environment does not change much, using automated suggestions based on the computation of common visibility. In a user study, we show that our method can reduce cybersickness when compared with manual speed control.</p>

<p><a name="Advertising Perception with Immersive Virtual Reality Devices"> </a></p>

<h4 id="advertising-perception-with-immersive-virtual-reality-devices">Advertising Perception with Immersive Virtual Reality Devices</h4>

<p><em>Eduardo Zilles Borba and Marcelo Knorich Zuffo</em></p>

<p>Abstract: This poster presents an initial study about people experience with advertising messages in Virtual Reality (VR) that simulates the urban space. Besides looking to the plastic and textual factors perceived by the users in the Virtual Environment (VE), this work also reflects about effects of immersion provided by different technological devices and its possible influences in the advertising message reception process – a head-mounted display (Oculus Rift DK2), a cavern automatic virtual environment (CAVE) and a desktop monitor (PC). To carry this empirical experiment, a 3D scenario that simulates a real city urban space was created and several advertising image formats were inserted on its landscape. User navigation through the urban space was designed in a first-person perspective. In short, we intend to accomplish two objectives: a) to identify which factors lead people to pay attention to adverting in immersive VE; b) to verify the immersion effects produced by different VR interfaces in the perception of advertising.</p>

<p><a name="Exploring Non-reversing Magic Mirrors for Screen-Based Augmented Reality Systems"> </a></p>

<h4 id="exploring-non-reversing-magic-mirrors-for-screen-based-augmented-reality-systems">Exploring Non-reversing Magic Mirrors for Screen-Based Augmented Reality Systems</h4>

<p><em>Felix Bork, Roghayeh Barmaki, Ulrich Eck, Pascal Fallavolita, Bernhard Fuerst, and Nassir Navab</em></p>

<p>Abstract: Screen-based Augmented Reality (AR) systems can be built as a window into the real world as often done in mobile AR applications or using the Magic Mirror metaphor, where users can see themselves with augmented graphics on a large display. The term Magic Mirror implies that the display shows the users enantiomorph, i.e. the mirror image, such that the system mimics a real-world physical mirror. However, the question arises whether one should design a traditional mirror, or instead display the true mirror image by means of a non-reversing mirror? We discuss the perceptual differences between these two mirror visualization concepts and present a first comparative study in the context of Magic Mirror anatomy teaching.</p>

<p><a name="Towards Understanding Scene Transition Techniques in Immersive 360 Movies and Cinematic Experiences"> </a></p>

<h4 id="towards-understanding-scene-transition-techniques-in-immersive-360-movies-and-cinematic-experiences">Towards Understanding Scene Transition Techniques in Immersive 360 Movies and Cinematic Experiences</h4>

<p><em>Kasra Rahimi Moghadam and Eric D. Ragan</em></p>

<p>Abstract: Many researchers have studied methods of effective travel in virtual environments, but little work has considered scene transitions, which may be important for virtual reality experiences like immersive 360 degree movies. In this research, we designed and evaluated three different scene transition techniques in two environments, conducted a pilot study, and collected metrics related to sickness, spatial orientation, and preference. Our preliminary results indicate that faster techniques are generally preferred by gamers and more gradual transitions are preferred by participants with less experience with 3D gaming and virtual reality.</p>

<p><a name="Coordinating Attention and Cooperation in Multi-user Virtual Reality Narratives"> </a></p>

<h4 id="coordinating-attention-and-cooperation-in-multi-user-virtual-reality-narratives">Coordinating Attention and Cooperation in Multi-user Virtual Reality Narratives</h4>

<p><em>Cullen Brown, Ghanshyam Bhutra, Mohamed Suhail, Qinghong Xu, and Eric D. Ragan</em></p>

<p>Abstract: Limited research has been performed attempting to handle multi-user storytelling environments in virtual reality. As such, a number of questions about handling story progression and maintaining user presence in a multi-user virtual environment have yet to be answered. We created a multi-user virtual reality story experience in which we intend to study a set of guided camera techniques and a set of gaze distractor techniques to determine how best to attract disparate users to the same story. Additionally, we describe our preliminary work and plans to study the effectiveness of these techniques, their effect on user presence, and generally how multiple users feel their actions affect the outcome of a story.</p>

<p><a name="Designing Intentional Impossible Spaces in Virtual Reality Narratives: A Case Study"> </a></p>

<h4 id="designing-intentional-impossible-spaces-in-virtual-reality-narratives-a-case-study">Designing Intentional Impossible Spaces in Virtual Reality Narratives: A Case Study</h4>

<p><em>Joshua A. Fisher, Amit Garg, Karan Pratap Singh, and Wesley Wang</em></p>

<p>Abstract: Natural movement and locomotion in Virtual Environments (VE) is constrained by the user’s immediate physical space. To overcome this obstacle, researchers have established the use of impossible spaces. This work illustrates how impossible spaces can be utilized to enhance the aesthetics of, and presence within, an interactive narrative. This is done by creating impossible spaces with a narrative intent. First, locomotion and impossible spaces in VR are surveyed; second, the benefits of using intentional impossible spaces from a narrative design perspective is presented; third, a VR narrative called Ares is put forth as a prototype; and fourth, a user study is explored. Impossible spaces with a narrative intent intertwines narratology with the world’s aesthetics to enhance dramatic agency.</p>

<p><a name="Comparison of a Speech-Based and a Pie-Menu-Based Interaction Metaphor for Application Control"> </a></p>

<h4 id="comparison-of-a-speech-based-and-a-pie-menu-based-interaction-metaphor-for-application-control">Comparison of a Speech-Based and a Pie-Menu-Based Interaction Metaphor for Application Control</h4>

<p><em>Sebastian Pick, Andrew S. Puika, and Torsten W. Kuhlen</em></p>

<p>Abstract: Choosing an adequate system control technique is crucial to support complex interaction scenarios in virtual reality applications. In this work, we compare an existing hierarchical pie-menu-based approach with a speech-recognition-based one in terms of task performance and user experience in a formal user study. As testbed, we use a factory planning application featuring a large set of system control options.</p>

<p><a name="Virginia Tech's Study Hall: A Virtual Method of Loci Mnemotechnic Study using a Neurologically-Based, Mechanism-Driven, Approach to Immersive Learning Research"> </a></p>

<h4 id="virginia-techs-study-hall-a-virtual-method-of-loci-mnemotechnic-study-using-a-neurologically-based-mechanism-driven-approach-to-immersive-learning-research">Virginia Tech’s Study Hall: A Virtual Method of Loci Mnemotechnic Study using a Neurologically-Based, Mechanism-Driven, Approach to Immersive Learning Research</h4>

<p><em>Jessie Mann, Nicholas Polys, Rachel Diana, Manasa Ananth, Brad Herald, and Sweetuben Platel</em></p>

<p>Abstract: The design of Virginia Tech’s (VT) Study Hall emerges from the current cognitive neuroscience understanding of memory as a spatially mediated encoding process. The driving questions are: Does the sense of spatial navigation generated by an immersive virtual experience aid in memory formation? Does virtual spatial navigation, when paired with learning cues, enhance information encoding relative to nonspatial and nonvirtual processes? A pilot study was executed comparing recall on non-navigational memorization processes to processes involving mental and virtual navigation and we are currently running a full study to see if we can replicate these effects with a more demanding memory task and refined study design.</p>

<p><a name="REINVENT: A Low-Cost, Virtual Reality Brain-Computer Interface for Severe Stroke Upper Limb Motor Recovery"> </a></p>

<h4 id="reinvent-a-low-cost-virtual-reality-brain-computer-interface-for-severe-stroke-upper-limb-motor-recovery">REINVENT: A Low-Cost, Virtual Reality Brain-Computer Interface for Severe Stroke Upper Limb Motor Recovery</h4>

<p><em>Ryan Spicer, Julia Anglin, David M. Krum, and Sook-Lei Liew</em></p>

<p>Abstract: There are few effective treatments for rehabilitation of severe motor impairment after stroke. We developed a novel closed-loop neurofeedback system called REINVENT to promote motor recovery in this population. REINVENT (Rehabilitation Environment using the Integration of Neuromuscular-based Virtual Enhancements for Neural Training) harnesses recent advances in neuroscience, wearable sensors, and virtual technology and integrates low-cost electroencephalography (EEG) and electromyography (EMG) sensors with feedback in a head-mounted virtual reality display (VR) to provide neurofeedback when an individual’s neuromuscular signals indicate movement attempt, even in the absence of actual movement. Here we describe the REINVENT prototype and provide evidence of the feasibility and safety of using REINVENT with older adults.</p>

<p><a name="Simulating Anthropomorphic Upper Body Actions in Virtual Reality using Head and Hand Motion Data"> </a></p>

<h4 id="simulating-anthropomorphic-upper-body-actions-in-virtual-reality-using-head-and-hand-motion-data">Simulating Anthropomorphic Upper Body Actions in Virtual Reality using Head and Hand Motion Data</h4>

<p><em>Dustin T. Han, Shyam Prathish Sargunam, and Eric D. Ragan</em></p>

<p>Abstract: The use of self avatars in virtual reality (VR) can bring users a stronger sense of presence and produce a more compelling experience by providing additional visual feedback during interactions. Avatars also become increasingly more relevant in VR as they provide a user with an identity for social interactions in multi-user settings. However, with current consumer VR setups that include only a head mounted display and hand controllers, implementation of self avatars are generally limited in the ability to mimic actions performed in the real world. Our work explores the idea of simulating a wide range of upper body motions using motion and positional data from only the head and hand motion data. We present a method to differentiate head and hip motions using information from captured motion data and applying corresponding changes to a virtual avatar. We discuss our approach and initial results.</p>

<p><a name="Contextualizing Construction Accident Reports in Virtual Environments for Safety Education"> </a></p>

<h4 id="contextualizing-construction-accident-reports-in-virtual-environments-for-safety-education">Contextualizing Construction Accident Reports in Virtual Environments for Safety Education</h4>

<p><em>Alyssa M. Peña and Eric D. Ragan</em></p>

<p>Abstract: Safety education is important in the construction industry. While research has been done on virtual environments for construction safety education, there is no set method for effectively contextualizing safety information and engaging students. In this research, we study the design of virtual environments to represent construction accident reports provided by the Occupational Health and Safety Administration (OSHA). We looked at different designs to contextualize the report data through space, visuals, and text. Users can explore the environment and interact through immersive virtual reality to learn more about a particular accident.</p>

<p><a name="Classification Method of Tactile Feeling using Stacked Autoencoder Based on Haptic Primary Colors"> </a></p>

<h4 id="classification-method-of-tactile-feeling-using-stacked-autoencoder-based-on-haptic-primary-colors">Classification Method of Tactile Feeling using Stacked Autoencoder Based on Haptic Primary Colors</h4>

<p><em>Fumihiro Kato, Charith Lasantha Fernando, Yasuyuki Inoue, and Susumu Tachi</em></p>

<p>Abstract: We have developed a classification method of tactile feeling using a stacked autoencoder-based neural network on haptic primary colors. The haptic primary colors principle is a concept of decomposing the human sensation of tactile feeling into force, vibration, and temperature. Images were obtained from variation in the frequency of the time series of the tactile feeling obtained when tracing a surface of an object, features were extracted by employing a stacked autoencoder using a neural network with two hidden layers, and supervised learning was conducted. We confirmed that the tactile feeling for three different surface materials can be classified with an accuracy of 82.0[%].</p>

<p><a name="Uni-CAVE: A Unity3D Plugin for Non-head Mounted VR Display Systems"> </a></p>

<h4 id="uni-cave-a-unity3d-plugin-for-non-head-mounted-vr-display-systems">Uni-CAVE: A Unity3D Plugin for Non-head Mounted VR Display Systems</h4>

<p><em>Ross Tredinnick, Brady Boettcher, Simon Smith, Samuel Solovy, and Kevin Ponto</em></p>

<p>Abstract: Unity3D has become a popular, freely available 3D game engine for design and construction of virtual environments. Unfortunately, the few options that currently exist for adapting Unity3D to distributed immersive tiled or projection-based VR display systems rely on closed commercial products. Uni-CAVE aims to solve this problem by creating a freely-available and easy to use Unity3D extension package for cluster-based VR display systems. This extension provides support for head and device tracking, stereo rendering and display synchronization. Furthermore, Uni-CAVE enables configuration within the Unity environment enabling researchers to get quickly up and running.</p>

<p><a name="3D Action Reconstruction using Virtual Player to Assist Karate Training"> </a></p>

<h4 id="d-action-reconstruction-using-virtual-player-to-assist-karate-training">3D Action Reconstruction using Virtual Player to Assist Karate Training</h4>

<p><em>Kazumoto Tanaka</em></p>

<p>Abstract: It is well known that sport skill learning is facilitated by video observation of players’ actions in the target sport. A viewpoint change function is desirable when a learner observes the actions using video images. However, in general, viewpoint changes for observation are not possible because most videos are filmed from a fixed point using a single video camera. The objective of this research is to develop a method that generates a 3D human model of a player (i.e., a virtual player) from a single image and enable observation of the virtual player's action from any point of view. As a first step, this study focused on karate training and developed a semiautomatic method for 3D reconstruction from video images of sparring in karate.</p>

<p><a name="Immersion and Coherence in a Visual Cliff Environment"> </a></p>

<h4 id="immersion-and-coherence-in-a-visual-cliff-environment">Immersion and Coherence in a Visual Cliff Environment</h4>

<p><em>Richard Skarbez, Frederick P. Brooks Jr., and Mary C. Whitton</em></p>

<p>Abstract: We report on the design and results of an experiment investigating Slater's Place Illusion (PI) and Plausibility Illusion (Psi) in a virtual visual cliff environment. Existing presence questionnaires could not reliably distinguish the effects of PI from those of Psi. They did, however, indicate that high levels of PI-eliciting characteristics and Psi-eliciting characteristics &#60;em&#62;together&#60;/em&#62; result in higher presence, compared to any of the other three conditions. Also, participants' heart rates responded markedly differently in the two Psi conditions; no such difference was observed across the PI conditions.</p>

<p><a name="Anatomy Builder VR: Applying a Constructive Learning Method in the Virtual Reality Canine Skeletal System"> </a></p>

<h4 id="anatomy-builder-vr-applying-a-constructive-learning-method-in-the-virtual-reality-canine-skeletal-system">Anatomy Builder VR: Applying a Constructive Learning Method in the Virtual Reality Canine Skeletal System</h4>

<p><em>Jinsil Hwaryoung Seo, Brian Smith, Margaret Cook, Michelle Pine, Erica Malone, Steven Leal, and Jinkyo Suh</em></p>

<p>Abstract: We present Anatomy Builder VR that examines how a virtual reality (VR) system can support embodied learning in anatomy education. The backbone of the project is to pursue an alternative constructivist pedagogical model for learning canine anatomy. The main focus of the study was to identify and assemble bones in the live-animal orientation, using real thoracic limb bones in a bone box and digital pelvic limb bones in the Anatomy Builder VR. Eleven college students participated in the study. The pilot study showed that participants most enjoyed interacting with anatomical contents within the VR program. Participants spent less time assembling bones in the VR, and instead spent a longer time tuning the orientation of each VR bone in the 3D space. This study showed how a constructivist method could support anatomy education while using virtual reality technology in an active and experiential way.</p>

<p><a name="Transfer of a Skilled Motor Learning Task between Virtual and Conventional Environments"> </a></p>

<h4 id="transfer-of-a-skilled-motor-learning-task-between-virtual-and-conventional-environments">Transfer of a Skilled Motor Learning Task between Virtual and Conventional Environments</h4>

<p><em>Julia Anglin, David Saldana, Allie Schmiesing, and Sook-Lei Liew</em></p>

<p>Abstract: Immersive, head-mounted virtual reality (HMD-VR) can be a potentially useful tool for motor rehabilitation. However, it is unclear whether the motor skills learned in HMD-VR transfer to the non-virtual world and vice-versa. Here we used a well-established test of skilled motor learning, the Sequential Visual Isometric Pinch Task (SVIPT), to train individuals in either an HMD-VR or conventional training (CT) environment. Participants were then tested in both environments. Our results show that participants who train in the CT environment have an improvement in motor performance when they transfer to the HMD-VR environment. In contrast, participants who train in the HMD-VR environment show a decrease in skill level when transferring to the CT environment. This has implications for how training in HMD-VR and CT may affect performance in different environments.</p>

<p><a name="A Methodology for Optimized Generation of Virtual Environments Based on Hydroelectric Power Plants"> </a></p>

<h4 id="a-methodology-for-optimized-generation-of-virtual-environments-based-on-hydroelectric-power-plants">A Methodology for Optimized Generation of Virtual Environments Based on Hydroelectric Power Plants</h4>

<p><em>Ígor Andrade Moraes, Alexandre Cardoso, Edgard Lamounier Jr., Milton Miranda Neto, and Isabela Cristina dos Santos Peres</em></p>

<p>Abstract: The profits and benefits offered by Virtual Reality technology had drawn attention of professionals from several scientific fields, including the power systems’, either for training or maintenance. For this purpose, 3D modeling is evidently pointed out as an imperative process for the conception of a Virtual Environment. Before the complexity of Hydroelectric Power Plants and Virtual Reality’s contribution for the Industrial area, planning the tridimensional construction of the virtual environment becomes necessary. Thus, this paper presents modeling techniques applicable to several hydroelectric structures, aiming to optimize the 3D construction of the target complexes.</p>

          </div>
          <div class="col-lg-2 col-lg-height col-md-height hidden-md hidden-sm hidden-xs" style="background-color: #000; height: 450px;"> &nbsp;</div>
        </div>
      </div>
    </div>
    <!-- Sponsor and social media (or other "float left column" content lives here for SM and XS windows)-->
    <!--
    <div class="row visible-sm visible-xs">
    <div class="col-sm-12">
    <h2>Sponsors</h2>
    <p>Sponsor images and links live here</p>
  </div>
</div>-->
</div>

<div id="footer">
  <div class="container">
    <p>
      &copy; 2017 webchairs2017 [at] ieeevr.org
    </p>
  </div>
</div>






<!-- Latest compiled and minified JavaScript, requires jQuery 1.x (2.x not supported in IE8) -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<!--<script src="/2017/assets/themes/bootstrap/js/bootstrap.min.js"></script>-->
<script src="/2017/assets/javascripts/bootstrap.min.js"></script>
</body>
</html>

